{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fft = librosa.get_fftlib()\n",
    "class VoxIterableDataset(object):\n",
    "    def __init__(self, data_dir_dict, data_len_dict, config):        \n",
    "        with open(data_dir_dict['spk2utt_train_dict'], 'rb') as handle:\n",
    "            self.spk2utt_train_dict = pickle.load(handle)\n",
    "        with open(data_dir_dict['music_dict'], 'rb') as handle:\n",
    "            self.music_dict = pickle.load(handle)\n",
    "        with open(data_dir_dict['noise_dict'], 'rb') as handle:\n",
    "            self.noise_dict = pickle.load(handle)\n",
    "        with open(data_dir_dict['babble_dict'], 'rb') as handle:\n",
    "            self.babble_dict = pickle.load(handle)\n",
    "        with open(data_dir_dict['rir_dict'], 'rb') as handle:\n",
    "            self.rir_dict = pickle.load(handle)\n",
    "            \n",
    "        with open(data_len_dict['spk2utt_train_len'], 'rb') as handle:\n",
    "            self.spk2utt_train_len = pickle.load(handle)\n",
    "        with open(data_len_dict['music_len'], 'rb') as handle:\n",
    "            self.music_len = pickle.load(handle)\n",
    "        with open(data_len_dict['noise_len'], 'rb') as handle:\n",
    "            self.noise_len = pickle.load(handle)\n",
    "        with open(data_len_dict['babble_len'], 'rb') as handle:\n",
    "            self.babble_len = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "        self.random_spkrs_batchlist = None\n",
    "        self.ramdom_batch_len = None\n",
    "        self.random_noise_type = None\n",
    "        \n",
    "        \n",
    "        self.possible_babble_num = [3, 4, 5, 6, 7]\n",
    "        self.possible_babble_snr = [13, 15, 17, 20]\n",
    "        self.possible_noise_snr = [0, 5, 10, 15]\n",
    "        self.possible_music_snr = [5, 8, 10, 15]\n",
    "        \n",
    "        self.sr = config['sr']\n",
    "        self.repeats = config['repeats']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.extended_prefectch = config['extended_prefectch']\n",
    "        \n",
    "        self.mfcc_dim = 30\n",
    "        \n",
    "        # Auxiliary paras\n",
    "        self.multi_read_count = 0\n",
    "        self.preload_mem = False\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        assert len(self.ramdom_batch_len) == len(self.random_spkrs_batchlist)\n",
    "        try:\n",
    "            batch_frame_len = self.ramdom_batch_len.pop(0)\n",
    "            batch_spkrs = self.random_spkrs_batchlist.pop(0)\n",
    "            batch_noise_type = self.random_noise_type.pop(0)\n",
    "            batched_feats = np.zeros([self.batch_size, batch_frame_len, self.mfcc_dim])\n",
    "            batched_labels = np.zeros(self.batch_size)\n",
    "            \n",
    "            for batch_index, (spkr, noise_type) in enumerate(zip(batch_spkrs, batch_noise_type)):\n",
    "                \n",
    "                concat_wav, VAD_result = self._colleting_and_slicing(spkr, batch_frame_len,\\\n",
    "                hop_len=160, extended_prefectch=self.extended_prefectch)\n",
    "            \n",
    "                \n",
    "                if noise_type == 0:\n",
    "                    aug_wav = concat_wav\n",
    "                \n",
    "                elif noise_type == 1:\n",
    "                    aug_wav = self._add_rebverb(concat_wav)\n",
    "                   \n",
    "                elif noise_type == 2:\n",
    "                    aug_wav = self._add_noise(concat_wav)\n",
    "                    \n",
    "                elif noise_type == 3:\n",
    "                    aug_wav = self._add_music(concat_wav)\n",
    "                  \n",
    "                elif noise_type == 4:\n",
    "                    aug_wav = self._add_babble(concat_wav)\n",
    "             \n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                    \n",
    "            \n",
    "                single_feats = librosa.feature.mfcc(y=aug_wav, sr=self.sr, n_mfcc=30, \\\n",
    "                dct_type=2, n_fft=512, hop_length=160, \\\n",
    "                win_length=None, window='hann', power=2.0, \\\n",
    "                center=True, pad_mode='reflect', n_mels=30, \\\n",
    "                fmin=20, fmax=7600)\n",
    "                # Note single_feats needs transpose\n",
    "                out_feats = self._CMVN(single_feats.T, cmn_window = 300, normalize_variance = False)\n",
    "                # Apply VAD\n",
    "                assert out_feats.shape[0] == VAD_result.shape[0]\n",
    "                out_feats = out_feats[VAD_result.astype(np.bool)]\n",
    "                batched_feats[batch_index] = out_feats[:batch_frame_len]\n",
    "                batched_labels[batch_index] = spkr\n",
    "                \n",
    "            return batched_feats, batched_labels\n",
    "        \n",
    "        except IndexError:\n",
    "            raise StopIteration\n",
    "\n",
    "    def process_one_utt(self, utt_dir):\n",
    "        try:\n",
    "            concat_wav, _ = librosa.load(utt_dir, sr=self.sr)\n",
    "            \n",
    "            VAD_result = self._VAD_detection(concat_wav)\n",
    "            \n",
    "            aug_wav = concat_wav\n",
    "\n",
    "            single_feats = librosa.feature.mfcc(y=aug_wav, sr=self.sr, n_mfcc=30, \\\n",
    "            dct_type=2, n_fft=512, hop_length=160, \\\n",
    "            win_length=None, window='hann', power=2.0, \\\n",
    "            center=True, pad_mode='reflect', n_mels=30, \\\n",
    "            fmin=20, fmax=7600)\n",
    "            # Note single_feats needs transpose\n",
    "            out_feats = self._CMVN(single_feats.T, cmn_window = 300, normalize_variance = False)\n",
    "            # Apply VAD\n",
    "            assert out_feats.shape[0] == VAD_result.shape[0]\n",
    "            out_feats = out_feats[VAD_result.astype(np.bool)]\n",
    "            \n",
    "            batched_feats = out_feats[None, :, :]\n",
    "                \n",
    "            return batched_feats\n",
    "        \n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def process_one_utt_noVAD(self, utt_dir):\n",
    "        try:\n",
    "            concat_wav, _ = librosa.load(utt_dir, sr=self.sr)\n",
    "            \n",
    "            aug_wav = concat_wav\n",
    "\n",
    "            single_feats = librosa.feature.mfcc(y=aug_wav, sr=self.sr, n_mfcc=30, \\\n",
    "            dct_type=2, n_fft=512, hop_length=160, \\\n",
    "            win_length=None, window='hann', power=2.0, \\\n",
    "            center=True, pad_mode='reflect', n_mels=30, \\\n",
    "            fmin=20, fmax=7600)\n",
    "            # Note single_feats needs transpose\n",
    "            out_feats = self._CMVN(single_feats.T, cmn_window = 300, normalize_variance = False)\n",
    "            \n",
    "            batched_feats = out_feats[None, :, :]\n",
    "                \n",
    "            return batched_feats\n",
    "        \n",
    "        except Exception:\n",
    "            traceback.print_exc()            \n",
    "    \n",
    "    def noise_data_preload(self):\n",
    "        print('preloading music_dict')\n",
    "        for count, i in enumerate(self.music_dict):\n",
    "            _, _ = librosa.load(self.music_dict[i], sr=self.sr)\n",
    "            if (count+1)%100 == 0:\n",
    "                print(count+1)\n",
    "        print('preloading noise_dict')        \n",
    "        for count, i in enumerate(self.noise_dict):\n",
    "            _, _ = librosa.load(self.noise_dict[i], sr=self.sr)\n",
    "            if (count+1)%100 == 0:\n",
    "                print(count+1)\n",
    "        print('preloading babble_dict')        \n",
    "        for count, i in enumerate(self.babble_dict):\n",
    "            _, _ = librosa.load(self.babble_dict[i], sr=self.sr)\n",
    "            if (count+1)%100 == 0:\n",
    "                print(count+1)\n",
    "    \n",
    "    def noise_data_preload2mem(self):\n",
    "        print('preloading to memory')\n",
    "        \n",
    "        self.music_preload_dict = {}\n",
    "        self.noise_preload_dict = {}\n",
    "        self.babble_preload_dict = {}\n",
    "        self.preload_mem = True\n",
    "        print('preloading music_dict')\n",
    "        for count, i in enumerate(self.music_dict):\n",
    "            self.music_preload_dict[i], _ = librosa.load(self.music_dict[i], sr=self.sr)\n",
    "            if (count+1)%100 == 0:\n",
    "                print(count+1)\n",
    "        print('preloading noise_dict')        \n",
    "        for count, i in enumerate(self.noise_dict):\n",
    "            self.noise_preload_dict[i], _ = librosa.load(self.noise_dict[i], sr=self.sr)\n",
    "            if (count+1)%100 == 0:\n",
    "                print(count+1)\n",
    "        print('preloading babble_dict')        \n",
    "        for count, i in enumerate(self.babble_dict):\n",
    "            self.babble_preload_dict[i], _ = librosa.load(self.babble_dict[i], sr=self.sr)\n",
    "            if (count+1)%100 == 0:\n",
    "                print(count+1)       \n",
    "        \n",
    "        \n",
    "    def get_random_list(self):\n",
    "        spkrs_list = self.repeats * list(self.spk2utt_train_dict.keys())\n",
    "        random.shuffle(spkrs_list)\n",
    "        len_spkrs_list = len(spkrs_list)\n",
    "        self.random_spkrs_batchlist = [spkrs_list[i*self.batch_size:i*self.batch_size+self.batch_size]\\\n",
    "        for i in range(len_spkrs_list // self.batch_size)]\n",
    "        \n",
    "        self.ramdom_batch_len = [random.randint(200, 400) for i in range(len_spkrs_list // self.batch_size)]\n",
    "        \n",
    "        noise_type_list = [i%5 for i in range(len_spkrs_list)]\n",
    "\n",
    "        random.shuffle(noise_type_list)\n",
    "        self.random_noise_type = [noise_type_list[i*self.batch_size:i*self.batch_size+self.batch_size]\\\n",
    "        for i in range(len_spkrs_list // self.batch_size)]\n",
    "        \n",
    "        assert len(self.random_spkrs_batchlist) == len(self.ramdom_batch_len)\\\n",
    "        == len(self.random_noise_type)\n",
    "        \n",
    "    def _colleting_and_slicing(self, spkr, batch_frame_len, hop_len=160, extended_prefectch=2.0):\n",
    "        \n",
    "        least_wav_len = (batch_frame_len - 1) * hop_len\n",
    "        concat_utt = np.zeros(0)\n",
    "        valid_frames_len = 0\n",
    "        \n",
    "        # Use to count multi_read_count\n",
    "        get_count = 0\n",
    "\n",
    "        while valid_frames_len < batch_frame_len:\n",
    "            concat_utt = np.zeros(0)\n",
    "\n",
    "            utt_dir = self._get_random_spk_utt(spkr, self.spk2utt_train_dict)\n",
    "            utt_len = self.spk2utt_train_len[utt_dir]\n",
    "#             off = self._get_random_offset(least_wav_len, utt_len) / self.sr\n",
    "            off = self._get_random_offset(least_wav_len+extended_prefectch*self.sr, utt_len) / self.sr\n",
    "            dur = least_wav_len / self.sr + extended_prefectch\n",
    "            \n",
    "            utt_part, _ = librosa.load(utt_dir, sr=self.sr, offset=off, duration=dur)\n",
    "            \n",
    "            concat_utt = np.append(concat_utt, utt_part)\n",
    "            detected_frames = self._VAD_detection(concat_utt)\n",
    "            valid_frames_len = np.sum(detected_frames)\n",
    "\n",
    "            get_count += 1\n",
    "\n",
    "        if get_count > 1:\n",
    "            self.multi_read_count += 1\n",
    "\n",
    "        VAD_result = detected_frames\n",
    "        return concat_utt, VAD_result\n",
    "    \n",
    "    def _add_rebverb(self, in_wav):\n",
    "        power_before_reverb = in_wav.dot(in_wav) / len(in_wav)\n",
    "        shift_index = 0\n",
    "        signal = in_wav\n",
    "        filter_dir = self._get_random_noise(self.rir_dict)\n",
    "        filter, _ = librosa.load(filter_dir, sr=self.sr)\n",
    "        \n",
    "        signal_length = len(signal)\n",
    "        filter_length = len(filter)\n",
    "        output_length = signal_length + filter_length - 1\n",
    "        output = np.zeros(output_length)\n",
    "\n",
    "        fft_length = 2**np.ceil(np.log2(4 * filter_length)).astype(np.int)\n",
    "        block_length = fft_length - filter_length + 1\n",
    "\n",
    "\n",
    "        filter_padded = np.zeros(fft_length)\n",
    "        filter_padded[0:filter_length] = filter\n",
    "        filter_padded = fft.rfft(filter_padded)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(signal_length//block_length + 1):\n",
    "            process_length = min(block_length, signal_length - i * block_length);\n",
    "            signal_block_padded = np.zeros(fft_length)\n",
    "            signal_block_padded[0:process_length] = signal[i * block_length : i * block_length + process_length]\n",
    "            signal_block_padded = fft.rfft(signal_block_padded)\n",
    "\n",
    "            signal_block_padded = filter_padded * signal_block_padded\n",
    "\n",
    "            signal_block_padded = fft.irfft(signal_block_padded, n=fft_length)\n",
    "\n",
    "            if (i*block_length + fft_length) <= output_length:\n",
    "                output[i*block_length : i*block_length + fft_length] += signal_block_padded\n",
    "            else:\n",
    "                output[i*block_length : output_length] += signal_block_padded[:output_length-i*block_length]\n",
    "        \n",
    "        # shift with max index of filter\n",
    "        shift_index = np.argmax(filter)\n",
    "        \n",
    "        final_out = output[shift_index:shift_index+signal_length]\n",
    "        power_after_reverb = final_out.dot(final_out) / len(final_out)\n",
    "        final_out = np.sqrt(power_before_reverb/power_after_reverb) * final_out\n",
    "        out_wav = final_out\n",
    "        \n",
    "        return out_wav\n",
    "    \n",
    "    def _add_noise(self, in_wav):\n",
    "        power_before_reverb = in_wav.dot(in_wav) / len(in_wav)\n",
    "        shift_index = 0\n",
    "        signal = np.zeros(len(in_wav))\n",
    "        signal[:] = in_wav[:]\n",
    "        \n",
    "        signal_len = len(signal)\n",
    "        total_noise_len = 0\n",
    "        signal_off = 0\n",
    "        while total_noise_len < signal_len:\n",
    "            \n",
    "            noise_dir, noise_index = self._get_random_noise(self.noise_dict, return_index=True)\n",
    "            noise_len = self.noise_len[noise_index]\n",
    "            if noise_len > signal_len:\n",
    "                noise_off = self._get_random_offset(signal_len, noise_len)\n",
    "                total_noise_len += signal_len\n",
    "                if self.preload_mem:\n",
    "                    noise = self.noise_preload_dict[noise_index][noise_off:noise_off+signal_len]\n",
    "                else:\n",
    "                    noise, _ = librosa.load(noise_dir, sr=self.sr, offset=noise_off/self.sr,\\\n",
    "                    duration=signal_len/self.sr)\n",
    "                \n",
    "            else:\n",
    "                total_noise_len += noise_len\n",
    "                if self.preload_mem:\n",
    "                    noise = self.noise_preload_dict[noise_index]\n",
    "                else:\n",
    "                    noise, _ = librosa.load(noise_dir, sr=self.sr)\n",
    "                \n",
    "            snr_db = self.possible_noise_snr[random.randint(0, len(self.possible_noise_snr)-1)]\n",
    "        \n",
    "            signal = self._add_db(signal, noise, signal_off, snr_db, power_before_reverb)\n",
    "            \n",
    "            signal_off += len(noise)\n",
    "        \n",
    "        output = signal\n",
    "        final_out = output[shift_index:shift_index+signal_len]\n",
    "        power_after_reverb = final_out.dot(final_out) / len(final_out)\n",
    "        final_out = np.sqrt(power_before_reverb/power_after_reverb) * final_out\n",
    "        out_wav = final_out\n",
    "        \n",
    "        return out_wav\n",
    "    \n",
    "    def _add_music(self, in_wav):\n",
    "        power_before_reverb = in_wav.dot(in_wav) / len(in_wav)\n",
    "        shift_index = 0\n",
    "        signal = np.zeros(len(in_wav))\n",
    "        signal[:] = in_wav[:]\n",
    "        \n",
    "        signal_len = len(signal)\n",
    "        total_noise_len = 0\n",
    "        signal_off = 0\n",
    "        while total_noise_len < signal_len:\n",
    "            \n",
    "            noise_dir, noise_index = self._get_random_noise(self.music_dict, return_index=True)\n",
    "            noise_len = self.music_len[noise_index]\n",
    "            if noise_len > signal_len:\n",
    "                noise_off = self._get_random_offset(signal_len, noise_len)\n",
    "                total_noise_len += signal_len\n",
    "                if self.preload_mem:\n",
    "                    noise = self.music_preload_dict[noise_index][noise_off:noise_off+signal_len]\n",
    "                else:\n",
    "                    noise, _ = librosa.load(noise_dir, sr=self.sr, offset=noise_off/self.sr,\\\n",
    "                    duration=signal_len/self.sr)\n",
    "            else:\n",
    "                total_noise_len += noise_len\n",
    "                if self.preload_mem:\n",
    "                    noise = self.music_preload_dict[noise_index]\n",
    "                else:\n",
    "                    noise, _ = librosa.load(noise_dir, sr=self.sr)\n",
    "                \n",
    "            snr_db = self.possible_music_snr[random.randint(0, len(self.possible_music_snr)-1)]\n",
    "        \n",
    "            signal = self._add_db(signal, noise, signal_off, snr_db, power_before_reverb)\n",
    "            \n",
    "            signal_off += len(noise)\n",
    "        \n",
    "        output = signal\n",
    "        final_out = output[shift_index:shift_index+signal_len]\n",
    "        power_after_reverb = final_out.dot(final_out) / len(final_out)\n",
    "        final_out = np.sqrt(power_before_reverb/power_after_reverb) * final_out\n",
    "        out_wav = final_out\n",
    "        \n",
    "        return out_wav\n",
    "    \n",
    "    def _add_babble(self, in_wav):\n",
    "        power_before_reverb = in_wav.dot(in_wav) / len(in_wav)\n",
    "        shift_index = 0\n",
    "        signal = np.zeros(len(in_wav))\n",
    "        signal[:] = in_wav[:]\n",
    "        \n",
    "        signal_len = len(signal)\n",
    "        signal_off = 0\n",
    "        bg_spks_num = self.possible_babble_num[random.randint(0, len(self.possible_babble_num)-1)]    \n",
    "        for _ in range(bg_spks_num):            \n",
    "            noise_dir, noise_index = self._get_random_noise(self.babble_dict, return_index=True)\n",
    "            noise_len = self.babble_len[noise_index]\n",
    "            if noise_len > signal_len:\n",
    "                noise_off = self._get_random_offset(signal_len, noise_len)\n",
    "                if self.preload_mem:\n",
    "                    noise = self.babble_preload_dict[noise_index][noise_off:noise_off+signal_len]\n",
    "                else:\n",
    "                    noise, _ = librosa.load(noise_dir, sr=self.sr, offset=noise_off/self.sr,\\\n",
    "                    duration=signal_len/self.sr)\n",
    "            else:\n",
    "                if self.preload_mem:\n",
    "                    noise = self.babble_preload_dict[noise_index]\n",
    "                else:\n",
    "                    noise, _ = librosa.load(noise_dir, sr=self.sr)\n",
    "                \n",
    "            snr_db = self.possible_babble_snr[random.randint(0, len(self.possible_babble_snr)-1)]\n",
    "        \n",
    "            signal = self._add_db(signal, noise, signal_off, snr_db, power_before_reverb)\n",
    "            \n",
    "        output = signal\n",
    "        final_out = output[shift_index:shift_index+signal_len]\n",
    "        power_after_reverb = final_out.dot(final_out) / len(final_out)\n",
    "        final_out = np.sqrt(power_before_reverb/power_after_reverb) * final_out\n",
    "        out_wav = final_out\n",
    "        \n",
    "        return out_wav\n",
    "    \n",
    "    def _add_db(self, in_wav, noise, signal_off, snr_db, power_before_reverb):\n",
    "        signal = np.zeros(len(in_wav))\n",
    "        signal[:] = in_wav[:]\n",
    "\n",
    "        noise_power = noise.dot(noise) / len(noise)\n",
    "        scale_factor = np.sqrt(10**(-snr_db / 10) * power_before_reverb / noise_power)\n",
    "        noise = scale_factor * noise\n",
    "\n",
    "        add_length = min(len(noise), len(signal)-signal_off)\n",
    "        signal[signal_off:signal_off+add_length] += noise[:add_length]\n",
    "        out_wav = signal      \n",
    "        \n",
    "        return out_wav\n",
    "    \n",
    "    def _CMVN(self, in_feat, cmn_window = 300, normalize_variance = False):             \n",
    "        num_frames = in_feat.shape[0]\n",
    "        dim = in_feat.shape[1]\n",
    "        last_window_start = -1\n",
    "        last_window_end = -1\n",
    "        cur_sum = np.zeros(dim)\n",
    "        cur_sumsq = np.zeros(dim)\n",
    "\n",
    "        out_feat = np.zeros([num_frames, dim])\n",
    "\n",
    "        for t in range(num_frames):\n",
    "            window_start = 0\n",
    "            window_end = 0\n",
    "\n",
    "            window_start = t - int(cmn_window / 2)\n",
    "            window_end = window_start + cmn_window\n",
    "\n",
    "            if (window_start < 0):\n",
    "                window_end -= window_start\n",
    "                window_start = 0\n",
    "\n",
    "            if (window_end > num_frames):\n",
    "                window_start -= (window_end - num_frames)\n",
    "                window_end = num_frames\n",
    "                if (window_start < 0):\n",
    "                    window_start = 0\n",
    "\n",
    "            if (last_window_start == -1):\n",
    "                input_part = in_feat[window_start:window_end]\n",
    "                cur_sum = np.sum(input_part, axis=0, keepdims=False)\n",
    "                if normalize_variance:\n",
    "                    cur_sumsq = np.sum(input_part**2, axis=0, keepdims=False)\n",
    "            else:\n",
    "                if (window_start > last_window_start):\n",
    "                    frame_to_remove = in_feat[last_window_start]\n",
    "                    cur_sum -= frame_to_remove\n",
    "                    if normalize_variance:\n",
    "                        cur_sumsq -= frame_to_remove**2\n",
    "\n",
    "                if (window_end > last_window_end):\n",
    "                    frame_to_add = in_feat[last_window_end]\n",
    "                    cur_sum += frame_to_add\n",
    "                    if normalize_variance:\n",
    "                        cur_sumsq += frame_to_add**2\n",
    "\n",
    "            window_frames = window_end - window_start\n",
    "            last_window_start = window_start\n",
    "            last_window_end = window_end\n",
    "\n",
    "            out_feat[t] = in_feat[t] - (1.0 / window_frames) * cur_sum\n",
    "\n",
    "\n",
    "            if normalize_variance:\n",
    "                if (window_frames == 1):\n",
    "                    out_feat[t] = 0.0\n",
    "                else:\n",
    "                    variance = (1.0 / window_frames) * cur_sumsq - (1.0 / window_frames**2) * cur_sum**2\n",
    "                    variance = np.maximum(1.0e-10, variance)\n",
    "                    out_feat[t] /= variance**(0.5)\n",
    "                    \n",
    "        return out_feat\n",
    "\n",
    "    def _get_random_noise(self, noise_dict, return_index=False):\n",
    "        dict_len = len(noise_dict)\n",
    "        i = random.randint(0, dict_len-1)\n",
    "        noise_dir = noise_dict[i]\n",
    "        \n",
    "        if return_index:\n",
    "            return noise_dir, i\n",
    "        else:\n",
    "            return noise_dir\n",
    "    \n",
    "    def _get_random_spk_utt(self, spkr, spk2utt):\n",
    "        this_utts = spk2utt[spkr]\n",
    "        this_num_utts = len(this_utts)\n",
    "        i = random.randint(0, this_num_utts-1)\n",
    "        utt_dir = this_utts[i]\n",
    "        return utt_dir\n",
    "\n",
    "    def _get_random_offset(self, expected_length, utt_len):\n",
    "        if expected_length > utt_len:\n",
    "            return 0\n",
    "        \n",
    "        free_length = utt_len - expected_length\n",
    "        offset = random.randint(0, free_length)\n",
    "        return offset\n",
    "        \n",
    "    @property\n",
    "    def _VAD_config(self):\n",
    "        vad_energy_threshold = -3.0\n",
    "        vad_energy_mean_scale = 1.0\n",
    "        vad_frames_context = 0\n",
    "        vad_proportion_threshold = 0.12\n",
    "        \n",
    "        return vad_energy_threshold, vad_energy_mean_scale,\\\n",
    "        vad_frames_context, vad_proportion_threshold\n",
    "        \n",
    "        \n",
    "    def _VAD_detection(self, wav):\n",
    "        vad_energy_threshold, vad_energy_mean_scale,\\\n",
    "        vad_frames_context, vad_proportion_threshold = self._VAD_config\n",
    "        \n",
    "        y_tmp = np.pad(wav, int(512 // 2), mode='reflect')\n",
    "        y_tmp = librosa.util.frame(y_tmp, frame_length=512, hop_length=160)\n",
    "        y_log_energy = np.log(np.maximum(np.sum(y_tmp**2, axis=0), 1e-15))\n",
    "\n",
    "        T = len(y_log_energy)\n",
    "        output_voiced = np.zeros(T)\n",
    "        if (T == 0):\n",
    "            raise Exception(\"zero wave length\")\n",
    "\n",
    "        energy_threshold = vad_energy_threshold\n",
    "        if (vad_energy_mean_scale != 0.0):\n",
    "            assert(vad_energy_mean_scale > 0.0)\n",
    "            energy_threshold += vad_energy_mean_scale * np.sum(y_log_energy) / T\n",
    "\n",
    "\n",
    "        assert(vad_frames_context >= 0)\n",
    "        assert(vad_proportion_threshold > 0.0 and vad_proportion_threshold < 1.0);\n",
    "\n",
    "        for t in range(T):\n",
    "            num_count = 0\n",
    "            den_count = 0\n",
    "            context = vad_frames_context\n",
    "            for t2 in range(t - context, t + context+1):\n",
    "                if (t2 >= 0 and t2 < T):\n",
    "                    den_count+=1\n",
    "                    if (y_log_energy[t2] > energy_threshold):\n",
    "                        num_count+=1\n",
    "\n",
    "            if (num_count >= den_count * vad_proportion_threshold):\n",
    "                output_voiced[t] = 1.0\n",
    "            else:\n",
    "                output_voiced[t] = 0.0\n",
    "        \n",
    "        return output_voiced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OPT_INDEX = '/Lun0/zhiyong/dataset'\n",
    "data_dir_dict = {}\n",
    "\n",
    "# val\n",
    "data_dir_dict['spk2utt_train_dict'] = os.path.join(OPT_INDEX, 'spk2utt_val_dict')\n",
    "data_dir_dict['music_dict'] = os.path.join(OPT_INDEX, 'music_dict')\n",
    "data_dir_dict['noise_dict'] = os.path.join(OPT_INDEX, 'noise_dict')\n",
    "data_dir_dict['babble_dict'] = os.path.join(OPT_INDEX, 'babble_dict')\n",
    "data_dir_dict['rir_dict'] = os.path.join(OPT_INDEX, 'rir_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_len_dict = {}\n",
    "\n",
    "data_len_dict['spk2utt_train_len'] = os.path.join(OPT_INDEX, 'spk2utt_val_len')\n",
    "data_len_dict['music_len'] = os.path.join(OPT_INDEX, 'music_len')\n",
    "data_len_dict['noise_len'] = os.path.join(OPT_INDEX, 'noise_len')\n",
    "data_len_dict['babble_len'] = os.path.join(OPT_INDEX, 'babble_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list = glob.glob('/Lun0/zhiyong/SdSV_2020_deepmine/task2_train/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = {}\n",
    "with open('/Lun0/zhiyong/SdSV_2020_deepmine/task2_enrollment/docs/train_labels.txt', 'r') as f:\n",
    "    for count, line in enumerate(f):\n",
    "        if count == 0:\n",
    "            continuee(line)\n",
    "            continue\n",
    "        line = line[:-1]\n",
    "        utt, label = line.split\n",
    "        train_labels[utt] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110673"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "1779.1428241729736\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "\n",
    "config['sr'] = 16000\n",
    "config['repeats'] = None\n",
    "config['batch_size'] = None\n",
    "config['extended_prefectch'] = None\n",
    "\n",
    "# trial_dict_dir = '/Lun0/zhiyong/dataset/trial_dict'\n",
    "\n",
    "def trial_data_preload(dataset, i, train_list, train_labels):\n",
    "        \n",
    "    for i, line in enumerate(train_list):\n",
    "        data = dataset.process_one_utt(line)\n",
    "        utt_label = line.split('/')[-1][:-4]\n",
    "        label = train_labels[utt_label]\n",
    "        with open('/Lun0/zhiyong/SdSV_2020_deepmine/train_mfcc/'+utt_label, 'wb') as handle:\n",
    "            pickle.dump((data.astype(np.float16), [label]), handle)\n",
    "        if ((i+1) % 1000) == 0:    \n",
    "            print(i+1)\n",
    "\n",
    "dataset = VoxIterableDataset(data_dir_dict, data_len_dict, config)\n",
    "\n",
    "processes = [Process(target = trial_data_preload, args = (dataset, i, train_list, train_labels)) for i in range(1)]\n",
    "start_time = time.time()\n",
    "[p.start() for p in processes]\n",
    "joined = [p.join() for p in processes]\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process_enr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OPT_INDEX = '/Lun0/zhiyong/dataset'\n",
    "data_dir_dict = {}\n",
    "\n",
    "# val\n",
    "data_dir_dict['spk2utt_train_dict'] = os.path.join(OPT_INDEX, 'spk2utt_val_dict')\n",
    "data_dir_dict['music_dict'] = os.path.join(OPT_INDEX, 'music_dict')\n",
    "data_dir_dict['noise_dict'] = os.path.join(OPT_INDEX, 'noise_dict')\n",
    "data_dir_dict['babble_dict'] = os.path.join(OPT_INDEX, 'babble_dict')\n",
    "data_dir_dict['rir_dict'] = os.path.join(OPT_INDEX, 'rir_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_len_dict = {}\n",
    "\n",
    "data_len_dict['spk2utt_train_len'] = os.path.join(OPT_INDEX, 'spk2utt_val_len')\n",
    "data_len_dict['music_len'] = os.path.join(OPT_INDEX, 'music_len')\n",
    "data_len_dict['noise_len'] = os.path.join(OPT_INDEX, 'noise_len')\n",
    "data_len_dict['babble_len'] = os.path.join(OPT_INDEX, 'babble_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enr_list = glob.glob('/Lun0/zhiyong/SdSV_2020_deepmine/task2_enrollment/wav/enrollment/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110673"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "1779.1428241729736\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "\n",
    "config['sr'] = 16000\n",
    "config['repeats'] = None\n",
    "config['batch_size'] = None\n",
    "config['extended_prefectch'] = None\n",
    "\n",
    "# trial_dict_dir = '/Lun0/zhiyong/dataset/trial_dict'\n",
    "\n",
    "def trial_data_preload(dataset, i, enr_list):\n",
    "        \n",
    "    for i, line in enumerate(enr_list):\n",
    "        data = dataset.process_one_utt(line)\n",
    "        label = line.split('/')[-1][:-4]\n",
    "        with open('/Lun0/zhiyong/SdSV_2020_deepmine/enr_mfcc/'+label, 'wb') as handle:\n",
    "            pickle.dump((data.astype(np.float16), [label]), handle)\n",
    "        if ((i+1) % 1000) == 0:    \n",
    "            print(i+1)\n",
    "\n",
    "dataset = VoxIterableDataset(data_dir_dict, data_len_dict, config)\n",
    "\n",
    "processes = [Process(target = trial_data_preload, args = (dataset, i, enr_list)) for i in range(1)]\n",
    "start_time = time.time()\n",
    "[p.start() for p in processes]\n",
    "joined = [p.join() for p in processes]\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process_evl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OPT_INDEX = '/Lun0/zhiyong/dataset'\n",
    "data_dir_dict = {}\n",
    "\n",
    "# val\n",
    "data_dir_dict['spk2utt_train_dict'] = os.path.join(OPT_INDEX, 'spk2utt_val_dict')\n",
    "data_dir_dict['music_dict'] = os.path.join(OPT_INDEX, 'music_dict')\n",
    "data_dir_dict['noise_dict'] = os.path.join(OPT_INDEX, 'noise_dict')\n",
    "data_dir_dict['babble_dict'] = os.path.join(OPT_INDEX, 'babble_dict')\n",
    "data_dir_dict['rir_dict'] = os.path.join(OPT_INDEX, 'rir_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_len_dict = {}\n",
    "\n",
    "data_len_dict['spk2utt_train_len'] = os.path.join(OPT_INDEX, 'spk2utt_val_len')\n",
    "data_len_dict['music_len'] = os.path.join(OPT_INDEX, 'music_len')\n",
    "data_len_dict['noise_len'] = os.path.join(OPT_INDEX, 'noise_len')\n",
    "data_len_dict['babble_len'] = os.path.join(OPT_INDEX, 'babble_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evl_list = glob.glob('/Lun0/zhiyong/SdSV_2020_deepmine/evaluation/wav/evaluation/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69542"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "502.18626618385315\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "\n",
    "config['sr'] = 16000\n",
    "config['repeats'] = None\n",
    "config['batch_size'] = None\n",
    "config['extended_prefectch'] = None\n",
    "\n",
    "# trial_dict_dir = '/Lun0/zhiyong/dataset/trial_dict'\n",
    "\n",
    "def trial_data_preload(dataset, i, evl_list):\n",
    "        \n",
    "    for i, line in enumerate(evl_list):\n",
    "        data = dataset.process_one_utt(line)\n",
    "        label = line.split('/')[-1][:-4]\n",
    "        with open('/Lun0/zhiyong/SdSV_2020_deepmine/evl_mfcc/'+label, 'wb') as handle:\n",
    "            pickle.dump((data.astype(np.float16), [label]), handle)\n",
    "        if ((i+1) % 1000) == 0:    \n",
    "            print(i+1)\n",
    "\n",
    "dataset = VoxIterableDataset(data_dir_dict, data_len_dict, config)\n",
    "\n",
    "processes = [Process(target = trial_data_preload, args = (dataset, i, evl_list)) for i in range(1)]\n",
    "start_time = time.time()\n",
    "[p.start() for p in processes]\n",
    "joined = [p.join() for p in processes]\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make train index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = '/Lun0/zhiyong/SdSV_2020_deepmine/train_mfcc'\n",
    "expected_len = 85764\n",
    "workers = 1\n",
    "single_worker_len = int(expected_len / workers)\n",
    "output = '/Lun0/zhiyong/SdSV_2020_deepmine/train_mfcc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mfcc_list = glob.glob('/Lun0/zhiyong/SdSV_2020_deepmine/train_mfcc/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert expected_len == len(train_mfcc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open(output, 'w') as f:\n",
    "    for i in train_mfcc_list:\n",
    "        path = i\n",
    "        assert os.path.isfile(path)\n",
    "        f.write(path+'\\n')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85764"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make enr index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = '/Lun0/zhiyong/SdSV_2020_deepmine/enr_mfcc'\n",
    "expected_len = 110673\n",
    "workers = 1\n",
    "single_worker_len = int(expected_len / workers)\n",
    "output = '/Lun0/zhiyong/SdSV_2020_deepmine/enr_mfcc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enr_mfcc_list = glob.glob('/Lun0/zhiyong/SdSV_2020_deepmine/enr_mfcc/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert expected_len == len(enr_mfcc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open(output, 'w') as f:\n",
    "    for i in enr_mfcc_list:\n",
    "        path = i\n",
    "        assert os.path.isfile(path)\n",
    "        f.write(path+'\\n')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110673"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make evl index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = '/Lun0/zhiyong/SdSV_2020_deepmine/evl_mfcc'\n",
    "expected_len = 69542\n",
    "workers = 1\n",
    "single_worker_len = int(expected_len / workers)\n",
    "output = '/Lun0/zhiyong/SdSV_2020_deepmine/evl_mfcc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evl_mfcc_list = glob.glob('/Lun0/zhiyong/SdSV_2020_deepmine/evl_mfcc/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert expected_len == len(evl_mfcc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open(output, 'w') as f:\n",
    "    for i in evl_mfcc_list:\n",
    "        path = i\n",
    "        assert os.path.isfile(path)\n",
    "        f.write(path+'\\n')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69542"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./train')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# import vox_model_bank\n",
    "from train import train_model_new\n",
    "from train.read_data import *\n",
    "from train.my_dataloader import *\n",
    "# from sklearn.metrics import roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '/Lun2/rzz/kaldi-master/egs/zhiyong/sre19/exp/Xvector_SAP_nodilate_1L_long_cosanel_newloader_newdata_2(test)/ckpt/min_eer.model'\n",
    "model_id = 'Xvector_SAP_nodilate_1L'\n",
    "model_metric = 'AM_normfree_softmax_anneal_ce_head'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "component_dir = './sdsvc'\n",
    "PLDA_DIM = 370\n",
    "PLDA_DATA_NAME = 'plda_data'\n",
    "TEST_DATA_NAME = 'test_data_370'\n",
    "PLDA_PARA_NAME = 'plda_para_370'\n",
    "SCORING_PLDA_NAME = 'score_plda_370'\n",
    "SCORING_COSINE_NAME = 'score_cosine_370'\n",
    "ENR_DATA_NAME = 'enr_data'\n",
    "EVL_DATA_NAME = 'evl_data'\n",
    "TRAIN_DATA_NAME = 'train_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(component_dir):\n",
    "    os.makedirs(component_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU multiprocess for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_p = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_list = '/Lun0/zhiyong/dataset/plda_full_data.csv'\n",
    "train_list = '/Lun0/zhiyong/SdSV_2020_deepmine/train_mfcc.csv'\n",
    "# train_data = CSVDataSet(train_list)\n",
    "train_data = PickleDataSet(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_len = len(train_data)\n",
    "num_per_process = (85764 // num_p) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "class_list_new_m = manager.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8577\n",
      "8577\n",
      "8577\n",
      "8577\n",
      "8577\n",
      "8577\n",
      "8577\n",
      "8577\n",
      "8577\n",
      "8571\n"
     ]
    }
   ],
   "source": [
    "data_m = []\n",
    "# class_list_new_m = []\n",
    "for i in range(num_p):\n",
    "    data = torch.utils.data.Subset(train_data, np.arange(i*num_per_process, min((i+1)*num_per_process, train_data_len)))\n",
    "    data_m.append(data)\n",
    "    class_list_new_m.append({})\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature_m(i, train_data, class_list_new_m):\n",
    "#     train_list = '/Lun0/zhiyong/dataset/vox12_kaldi_train_data/vox12_kaldi_train_data.csv'\n",
    "    model_settings = {'in_feat': 30, 'emb_size': 512, 'class_num': 7323, 's': 50, 'm': 0.2, 'anneal_steps': 0, 'HistK_len': 0}\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if i < 26:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(i%2)\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "#     torch.cuda.set_device(i%2)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "#     train_data = CSVDataSet(train_list)\n",
    "#     train_dataloader = DataLoader(dataset=train_data, batch_size = 1, shuffle = False, num_workers = 32, pin_memory=False)\n",
    "\n",
    "    train_dataloader = My_DataLoader(train_data, batch_size=None, shuffle=False, sampler=None,\\\n",
    "    batch_sampler=None, num_workers=8, collate_fn=None,\\\n",
    "    pin_memory=False, drop_last=False, timeout=0,\\\n",
    "    worker_init_fn=None, multiprocessing_context=None)\n",
    "\n",
    "    model = train_model_new.get_model(model_id, model_metric, None, model_settings, None)\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'], strict=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    class_list_new = {}\n",
    "\n",
    "    for count, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_x = batch_x.to(device)\n",
    "        label = batch_y[0].split('-')[0]\n",
    "        batch_y = torch.tensor([0]).to(device)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                _, _, emb, _, _ = model(batch_x, batch_y, mod='eval')\n",
    "        except:\n",
    "            print('Proc', str(i), 'EER:', label)\n",
    "            continue\n",
    "    #     _, _, emb, _, _ = model(batch_x, batch_y, mod='eval')\n",
    "\n",
    "        emb = emb.squeeze().data.cpu().numpy()\n",
    "\n",
    "        if label not in class_list_new.keys():\n",
    "            class_list_new[label] = emb[None, :]\n",
    "        else:\n",
    "            class_list_new[label] = np.append(class_list_new[label], emb[None, :], axis=0)\n",
    "\n",
    "        if (count+1) % 10000 == 0:\n",
    "            print('Proc '+ str(i) + ':' + str((count+1) // 10000))\n",
    "    \n",
    "    class_list_new_m[i] = class_list_new\n",
    "    del model, batch_x, batch_y\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n"
     ]
    }
   ],
   "source": [
    "processes = [Process(target = extract_feature_m, args = (i, data_m[i], class_list_new_m)) for i in range(num_p)]\n",
    "[p.start() for p in processes]\n",
    "joined = [p.join() for p in processes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85764"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in class_list_new_m:\n",
    "    for j in i:\n",
    "        count += len(i[j])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_list_new = class_list_new_m[0]\n",
    "\n",
    "for count, this_list in enumerate(class_list_new_m):\n",
    "    if count == 0:\n",
    "        continue\n",
    "    for this_label in this_list:\n",
    "        if this_label not in class_list_new.keys():\n",
    "            class_list_new[this_label] = this_list[this_label]\n",
    "        else:\n",
    "            class_list_new[this_label] = np.append(class_list_new[this_label], this_list[this_label], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85764"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in class_list_new:\n",
    "    count += len(class_list_new[i])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nanlist = []\n",
    "for i in class_list_new:\n",
    "    if np.isnan(class_list_new[i]).any():\n",
    "        print(i)\n",
    "        nanlist.append(i)\n",
    "for i in nanlist:\n",
    "    class_list_new.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85764"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "out = component_dir+'/'+TRAIN_DATA_NAME\n",
    "with open(out, 'wb') as handle:\n",
    "    pickle.dump(class_list_new, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU multiprocess for enr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_p = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_list = '/Lun0/zhiyong/dataset/plda_full_data.csv'\n",
    "train_list = '/Lun0/zhiyong/SdSV_2020_deepmine/enr_mfcc.csv'\n",
    "# train_data = CSVDataSet(train_list)\n",
    "train_data = PickleDataSet(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_len = len(train_data)\n",
    "num_per_process = (110673 // num_p) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "class_list_new_m = manager.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11068\n",
      "11068\n",
      "11068\n",
      "11068\n",
      "11068\n",
      "11068\n",
      "11068\n",
      "11068\n",
      "11068\n",
      "11061\n"
     ]
    }
   ],
   "source": [
    "data_m = []\n",
    "# class_list_new_m = []\n",
    "for i in range(num_p):\n",
    "    data = torch.utils.data.Subset(train_data, np.arange(i*num_per_process, min((i+1)*num_per_process, train_data_len)))\n",
    "    data_m.append(data)\n",
    "    class_list_new_m.append({})\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature_m(i, train_data, class_list_new_m):\n",
    "#     train_list = '/Lun0/zhiyong/dataset/vox12_kaldi_train_data/vox12_kaldi_train_data.csv'\n",
    "    model_settings = {'in_feat': 30, 'emb_size': 512, 'class_num': 7323, 's': 50, 'm': 0.2, 'anneal_steps': 0, 'HistK_len': 0}\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if i < 26:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(i%2)\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "#     torch.cuda.set_device(i%2)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "#     train_data = CSVDataSet(train_list)\n",
    "#     train_dataloader = DataLoader(dataset=train_data, batch_size = 1, shuffle = False, num_workers = 32, pin_memory=False)\n",
    "\n",
    "    train_dataloader = My_DataLoader(train_data, batch_size=None, shuffle=False, sampler=None,\\\n",
    "    batch_sampler=None, num_workers=8, collate_fn=None,\\\n",
    "    pin_memory=False, drop_last=False, timeout=0,\\\n",
    "    worker_init_fn=None, multiprocessing_context=None)\n",
    "\n",
    "    model = train_model_new.get_model(model_id, model_metric, None, model_settings, None)\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'], strict=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    class_list_new = {}\n",
    "\n",
    "    for count, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_x = batch_x.to(device)\n",
    "        label = batch_y[0].split('-')[0]\n",
    "        batch_y = torch.tensor([0]).to(device)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                _, _, emb, _, _ = model(batch_x, batch_y, mod='eval')\n",
    "        except:\n",
    "            print('Proc', str(i), 'EER:', label)\n",
    "            continue\n",
    "    #     _, _, emb, _, _ = model(batch_x, batch_y, mod='eval')\n",
    "\n",
    "        emb = emb.squeeze().data.cpu().numpy()\n",
    "\n",
    "        if label not in class_list_new.keys():\n",
    "            class_list_new[label] = emb[None, :]\n",
    "        else:\n",
    "            class_list_new[label] = np.append(class_list_new[label], emb[None, :], axis=0)\n",
    "\n",
    "        if (count+1) % 10000 == 0:\n",
    "            print('Proc '+ str(i) + ':' + str((count+1) // 10000))\n",
    "    \n",
    "    class_list_new_m[i] = class_list_new\n",
    "    del model, batch_x, batch_y\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Proc 8:1\n",
      "Proc 2:1\n",
      "Proc 6:1\n",
      "Proc 0:1\n",
      "Proc 4:1\n",
      "Proc 7:1\n",
      "Proc 1:1\n",
      "Proc 9:1\n",
      "Proc 3:1\n",
      "Proc 5:1\n"
     ]
    }
   ],
   "source": [
    "processes = [Process(target = extract_feature_m, args = (i, data_m[i], class_list_new_m)) for i in range(num_p)]\n",
    "[p.start() for p in processes]\n",
    "joined = [p.join() for p in processes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110673"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in class_list_new_m:\n",
    "    for j in i:\n",
    "        count += len(i[j])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_list_new = class_list_new_m[0]\n",
    "\n",
    "for count, this_list in enumerate(class_list_new_m):\n",
    "    if count == 0:\n",
    "        continue\n",
    "    for this_label in this_list:\n",
    "        if this_label not in class_list_new.keys():\n",
    "            class_list_new[this_label] = this_list[this_label]\n",
    "        else:\n",
    "            class_list_new[this_label] = np.append(class_list_new[this_label], this_list[this_label], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110673"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in class_list_new:\n",
    "    count += len(class_list_new[i])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nanlist = []\n",
    "for i in class_list_new:\n",
    "    if np.isnan(class_list_new[i]).any():\n",
    "        print(i)\n",
    "        nanlist.append(i)\n",
    "for i in nanlist:\n",
    "    class_list_new.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110673"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110673"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "out = component_dir+'/'+ENR_DATA_NAME\n",
    "with open(out, 'wb') as handle:\n",
    "    pickle.dump(class_list_new, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU multiprocess for evl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_p = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_list = '/Lun0/zhiyong/dataset/plda_full_data.csv'\n",
    "train_list = '/Lun0/zhiyong/SdSV_2020_deepmine/evl_mfcc.csv'\n",
    "# train_data = CSVDataSet(train_list)\n",
    "train_data = PickleDataSet(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_len = len(train_data)\n",
    "num_per_process = (69542 // num_p) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "class_list_new_m = manager.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6955\n",
      "6955\n",
      "6955\n",
      "6955\n",
      "6955\n",
      "6955\n",
      "6955\n",
      "6955\n",
      "6955\n",
      "6947\n"
     ]
    }
   ],
   "source": [
    "data_m = []\n",
    "# class_list_new_m = []\n",
    "for i in range(num_p):\n",
    "    data = torch.utils.data.Subset(train_data, np.arange(i*num_per_process, min((i+1)*num_per_process, train_data_len)))\n",
    "    data_m.append(data)\n",
    "    class_list_new_m.append({})\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature_m(i, train_data, class_list_new_m):\n",
    "#     train_list = '/Lun0/zhiyong/dataset/vox12_kaldi_train_data/vox12_kaldi_train_data.csv'\n",
    "    model_settings = {'in_feat': 30, 'emb_size': 512, 'class_num': 7323, 's': 50, 'm': 0.2, 'anneal_steps': 0, 'HistK_len': 0}\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if i < 26:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(i%2)\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "#     torch.cuda.set_device(i%2)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "#     train_data = CSVDataSet(train_list)\n",
    "#     train_dataloader = DataLoader(dataset=train_data, batch_size = 1, shuffle = False, num_workers = 32, pin_memory=False)\n",
    "\n",
    "    train_dataloader = My_DataLoader(train_data, batch_size=None, shuffle=False, sampler=None,\\\n",
    "    batch_sampler=None, num_workers=8, collate_fn=None,\\\n",
    "    pin_memory=False, drop_last=False, timeout=0,\\\n",
    "    worker_init_fn=None, multiprocessing_context=None)\n",
    "\n",
    "    model = train_model_new.get_model(model_id, model_metric, None, model_settings, None)\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'], strict=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    class_list_new = {}\n",
    "\n",
    "    for count, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_x = batch_x.to(device)\n",
    "        label = batch_y[0].split('-')[0]\n",
    "        batch_y = torch.tensor([0]).to(device)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                _, _, emb, _, _ = model(batch_x, batch_y, mod='eval')\n",
    "        except:\n",
    "            print('Proc', str(i), 'EER:', label)\n",
    "            continue\n",
    "    #     _, _, emb, _, _ = model(batch_x, batch_y, mod='eval')\n",
    "\n",
    "        emb = emb.squeeze().data.cpu().numpy()\n",
    "\n",
    "        if label not in class_list_new.keys():\n",
    "            class_list_new[label] = emb[None, :]\n",
    "        else:\n",
    "            class_list_new[label] = np.append(class_list_new[label], emb[None, :], axis=0)\n",
    "\n",
    "        if (count+1) % 10000 == 0:\n",
    "            print('Proc '+ str(i) + ':' + str((count+1) // 10000))\n",
    "    \n",
    "    class_list_new_m[i] = class_list_new\n",
    "    del model, batch_x, batch_y\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n"
     ]
    }
   ],
   "source": [
    "processes = [Process(target = extract_feature_m, args = (i, data_m[i], class_list_new_m)) for i in range(num_p)]\n",
    "[p.start() for p in processes]\n",
    "joined = [p.join() for p in processes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69542"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in class_list_new_m:\n",
    "    for j in i:\n",
    "        count += len(i[j])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_list_new = class_list_new_m[0]\n",
    "\n",
    "for count, this_list in enumerate(class_list_new_m):\n",
    "    if count == 0:\n",
    "        continue\n",
    "    for this_label in this_list:\n",
    "        if this_label not in class_list_new.keys():\n",
    "            class_list_new[this_label] = this_list[this_label]\n",
    "        else:\n",
    "            class_list_new[this_label] = np.append(class_list_new[this_label], this_list[this_label], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69542"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in class_list_new:\n",
    "    count += len(class_list_new[i])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nanlist = []\n",
    "for i in class_list_new:\n",
    "    if np.isnan(class_list_new[i]).any():\n",
    "        print(i)\n",
    "        nanlist.append(i)\n",
    "for i in nanlist:\n",
    "    class_list_new.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69542"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69542"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "out = component_dir+'/'+EVL_DATA_NAME\n",
    "with open(out, 'wb') as handle:\n",
    "    pickle.dump(class_list_new, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU multiprocess for plda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_p = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list = '/Lun0/zhiyong/dataset/plda_full_data.csv'\n",
    "# train_list = '/Lun0/zhiyong/dataset/plda_full_data_noVAD.csv'\n",
    "# train_data = CSVDataSet(train_list)\n",
    "train_data = PickleDataSet(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_len = len(train_data)\n",
    "num_per_process = (1276888 // num_p) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "class_list_new_m = manager.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45604\n",
      "45580\n"
     ]
    }
   ],
   "source": [
    "data_m = []\n",
    "# class_list_new_m = []\n",
    "for i in range(num_p):\n",
    "    data = torch.utils.data.Subset(train_data, np.arange(i*num_per_process, min((i+1)*num_per_process, train_data_len)))\n",
    "    data_m.append(data)\n",
    "    class_list_new_m.append({})\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature_m(i, train_data, class_list_new_m):\n",
    "#     train_list = '/Lun0/zhiyong/dataset/vox12_kaldi_train_data/vox12_kaldi_train_data.csv'\n",
    "    model_settings = {'in_feat': 30, 'emb_size': 512, 'class_num': 7323, 's': 50, 'm': 0.2, 'anneal_steps': 0, 'HistK_len': 0}\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if i < 26:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(i%2)\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "#     torch.cuda.set_device(i%2)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "#     train_data = CSVDataSet(train_list)\n",
    "#     train_dataloader = DataLoader(dataset=train_data, batch_size = 1, shuffle = False, num_workers = 32, pin_memory=False)\n",
    "\n",
    "    train_dataloader = My_DataLoader(train_data, batch_size=None, shuffle=False, sampler=None,\\\n",
    "    batch_sampler=None, num_workers=8, collate_fn=None,\\\n",
    "    pin_memory=False, drop_last=False, timeout=0,\\\n",
    "    worker_init_fn=None, multiprocessing_context=None)\n",
    "\n",
    "    model = train_model_new.get_model(model_id, model_metric, None, model_settings, None)\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'], strict=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    class_list_new = {}\n",
    "\n",
    "    for count, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_x = batch_x.to(device)\n",
    "        label = batch_y[0].split('-')[0]\n",
    "        batch_y = torch.tensor([0]).to(device)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                _, _, emb, _, _ = model(batch_x, batch_y, mod='eval')\n",
    "        except:\n",
    "            print('Proc', str(i), 'EER:', label)\n",
    "            continue\n",
    "    #     _, _, emb, _, _ = model(batch_x, batch_y, mod='eval')\n",
    "\n",
    "        emb = emb.squeeze().data.cpu().numpy()\n",
    "\n",
    "        if label not in class_list_new.keys():\n",
    "            class_list_new[label] = emb[None, :]\n",
    "        else:\n",
    "            class_list_new[label] = np.append(class_list_new[label], emb[None, :], axis=0)\n",
    "\n",
    "        if (count+1) % 10000 == 0:\n",
    "            print('Proc '+ str(i) + ':' + str((count+1) // 10000))\n",
    "    \n",
    "    class_list_new_m[i] = class_list_new\n",
    "    del model, batch_x, batch_y\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Using new training model bank\n",
      "Proc 27:1\n",
      "Proc 24:1\n",
      "Proc 26:1\n",
      "Proc 22:1\n",
      "Proc 20:1\n",
      "Proc 16:1\n",
      "Proc 10:1\n",
      "Proc 6:1\n",
      "Proc 8:1\n",
      "Proc 18:1\n",
      "Proc 4:1\n",
      "Proc 25:1\n",
      "Proc 2:1\n",
      "Proc 12:1\n",
      "Proc 14:1\n",
      "Proc 21:1\n",
      "Proc 19:1\n",
      "Proc 0:1\n",
      "Proc 11:1\n",
      "Proc 23:1\n",
      "Proc 9:1\n",
      "Proc 15:1\n",
      "Proc 5:1\n",
      "Proc 7:1\n",
      "Proc 13:1\n",
      "Proc 3:1\n",
      "Proc 17:1\n",
      "Proc 1:1\n",
      "Proc 24:2\n",
      "Proc 27:2\n",
      "Proc 26:2\n",
      "Proc 20:2\n",
      "Proc 22:2\n",
      "Proc 18:2\n",
      "Proc 16:2\n",
      "Proc 8:2\n",
      "Proc 14:2\n",
      "Proc 12:2\n",
      "Proc 25:2\n",
      "Proc 10:2\n",
      "Proc 4:2\n",
      "Proc 6:2\n",
      "Proc 2:2\n",
      "Proc 21:2\n",
      "Proc 0:2\n",
      "Proc 19:2\n",
      "Proc 9:2\n",
      "Proc 11:2\n",
      "Proc 23:2\n",
      "Proc 17:2\n",
      "Proc 15:2\n",
      "Proc 7:2\n",
      "Proc 13:2\n",
      "Proc 5:2\n",
      "Proc 3:2\n",
      "Proc 1:2\n",
      "Proc 24:3\n",
      "Proc 26:3\n",
      "Proc 27:3\n",
      "Proc 22:3\n",
      "Proc 18:3\n",
      "Proc 20:3\n",
      "Proc 16:3\n",
      "Proc 14:3\n",
      "Proc 8:3\n",
      "Proc 12:3\n",
      "Proc 10:3\n",
      "Proc 4:3\n",
      "Proc 25:3\n",
      "Proc 6:3\n",
      "Proc 2:3\n",
      "Proc 0:3\n",
      "Proc 19:3\n",
      "Proc 21:3\n",
      "Proc 9:3\n",
      "Proc 23:3\n",
      "Proc 11:3\n",
      "Proc 15:3\n",
      "Proc 17:3\n",
      "Proc 7:3\n",
      "Proc 13:3\n",
      "Proc 5:3\n",
      "Proc 3:3\n",
      "Proc 1:3\n",
      "Proc 26:4\n",
      "Proc 24:4\n",
      "Proc 27:4\n",
      "Proc 22:4\n",
      "Proc 20:4\n",
      "Proc 18:4\n",
      "Proc 16:4\n",
      "Proc 14:4\n",
      "Proc 12:4\n",
      "Proc 8:4\n",
      "Proc 10:4\n",
      "Proc 25:4\n",
      "Proc 6:4\n",
      "Proc 4:4\n",
      "Proc 2:4\n",
      "Proc 0:4\n",
      "Proc 19:4\n",
      "Proc 21:4\n",
      "Proc 23:4\n",
      "Proc 15:4\n",
      "Proc 9:4\n",
      "Proc 11:4\n",
      "Proc 7:4\n",
      "Proc 13:4\n",
      "Proc 5:4\n",
      "Proc 17:4\n",
      "Proc 3:4\n",
      "Proc 1:4\n"
     ]
    }
   ],
   "source": [
    "processes = [Process(target = extract_feature_m, args = (i, data_m[i], class_list_new_m)) for i in range(num_p)]\n",
    "[p.start() for p in processes]\n",
    "joined = [p.join() for p in processes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1276888"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in class_list_new_m:\n",
    "    for j in i:\n",
    "        count += len(i[j])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_list_new = class_list_new_m[0]\n",
    "\n",
    "for count, this_list in enumerate(class_list_new_m):\n",
    "    if count == 0:\n",
    "        continue\n",
    "    for this_label in this_list:\n",
    "        if this_label not in class_list_new.keys():\n",
    "            class_list_new[this_label] = this_list[this_label]\n",
    "        else:\n",
    "            class_list_new[this_label] = np.append(class_list_new[this_label], this_list[this_label], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1276888"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in class_list_new:\n",
    "    count += len(class_list_new[i])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nanlist = []\n",
    "for i in class_list_new:\n",
    "    if np.isnan(class_list_new[i]).any():\n",
    "        print(i)\n",
    "        nanlist.append(i)\n",
    "for i in nanlist:\n",
    "    class_list_new.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1276888"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7323"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "out = component_dir+'/'+PLDA_DATA_NAME\n",
    "with open(out, 'wb') as handle:\n",
    "    pickle.dump(class_list_new, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class_list_new = {}\n",
    "# for i in class_list:\n",
    "#     class_list_new[i[:-1]] = class_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLDA_FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./train')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# import vox_model_bank\n",
    "from train import train_model_new\n",
    "from train.read_data import *\n",
    "from train.my_dataloader import *\n",
    "# from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from kaldi_plda import *\n",
    "from kaldi_lda import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Lun2/rzz/kaldi-master/egs/zhiyong/sre19/new_testbench'+'/'+PLDA_DATA_NAME, 'rb') as handle:\n",
    "    class_list_new = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of mean: 1.9705881360946418\n"
     ]
    }
   ],
   "source": [
    "# Substract global mean\n",
    "global_mean = np.zeros(512)\n",
    "num_utt = 0\n",
    "for count, i in enumerate(class_list_new):\n",
    "    num_utt += class_list_new[i].shape[0]\n",
    "    global_mean += class_list_new[i].shape[0] * np.mean(class_list_new[i], axis=0)\n",
    "    \n",
    "global_mean = (1.0 / num_utt) * global_mean\n",
    "print('Norm of mean:', np.linalg.norm(global_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in class_list_new:\n",
    "    class_list_new[i] = class_list_new[i] - global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # normlize to sqrt(dim)\n",
    "# for i in class_list_new:\n",
    "#     scale = np.sqrt(512) / np.linalg.norm(class_list_new[i], axis=1, keepdims=True)\n",
    "#     class_list_new[i] = scale * class_list_new[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda norm of global mean: 9.641873606430675e-08\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(lda_dim=PLDA_DIM, ivector_dim=512)\n",
    "for i in class_list_new:\n",
    "    lda.AccStats(class_list_new[i])\n",
    "print('lda norm of global mean:', lda.GetGlobalMean()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the input data has norm of mean 9.641873606430675e-08\n",
      "[7.01190490e+00 6.75681662e+00 6.53853053e+00 6.25568993e+00\n",
      " 6.11248365e+00 5.95853316e+00 5.76067349e+00 5.64741497e+00\n",
      " 5.57487744e+00 5.41990231e+00 5.38111534e+00 5.31902969e+00\n",
      " 5.23789065e+00 5.19969617e+00 5.15591191e+00 5.06367837e+00\n",
      " 4.95002558e+00 4.87409618e+00 4.84840474e+00 4.74871468e+00\n",
      " 4.71036534e+00 4.62632340e+00 4.59444607e+00 4.53859824e+00\n",
      " 4.50825004e+00 4.44792223e+00 4.42513736e+00 4.34977219e+00\n",
      " 4.28159505e+00 4.25209140e+00 4.19223498e+00 4.16862494e+00\n",
      " 4.06981441e+00 4.06355494e+00 4.03968798e+00 3.96515890e+00\n",
      " 3.92289551e+00 3.89805072e+00 3.86241858e+00 3.79790583e+00\n",
      " 3.72304290e+00 3.67162826e+00 3.64237800e+00 3.57782648e+00\n",
      " 3.55594916e+00 3.51338202e+00 3.48159699e+00 3.44671244e+00\n",
      " 3.38799819e+00 3.37222889e+00 3.31932983e+00 3.26479589e+00\n",
      " 3.23583721e+00 3.18890552e+00 3.16877421e+00 3.13774669e+00\n",
      " 3.09139334e+00 3.04371133e+00 3.00723616e+00 2.97468080e+00\n",
      " 2.93475707e+00 2.91202466e+00 2.88878275e+00 2.87596840e+00\n",
      " 2.86439953e+00 2.80306845e+00 2.76811226e+00 2.75282091e+00\n",
      " 2.73265968e+00 2.70924597e+00 2.66613388e+00 2.63483158e+00\n",
      " 2.61028253e+00 2.58592747e+00 2.53934771e+00 2.51142099e+00\n",
      " 2.49924999e+00 2.49027539e+00 2.47251919e+00 2.42728511e+00\n",
      " 2.39750196e+00 2.37405700e+00 2.35193609e+00 2.32716667e+00\n",
      " 2.30830347e+00 2.25238815e+00 2.22747853e+00 2.19589194e+00\n",
      " 2.18315646e+00 2.16168723e+00 2.14473982e+00 2.13666842e+00\n",
      " 2.11730956e+00 2.09368117e+00 2.07249359e+00 2.02863364e+00\n",
      " 2.00766347e+00 1.98834752e+00 1.97887775e+00 1.94679614e+00\n",
      " 1.93039694e+00 1.86316743e+00 1.83820799e+00 1.80521765e+00\n",
      " 1.80025135e+00 1.73680036e+00 1.72558673e+00 1.69926713e+00\n",
      " 1.68113125e+00 1.64568037e+00 1.64385487e+00 1.61517082e+00\n",
      " 1.59876115e+00 1.58277031e+00 1.56115538e+00 1.55320346e+00\n",
      " 1.51243525e+00 1.49619275e+00 1.47218505e+00 1.43834861e+00\n",
      " 1.40241336e+00 1.39010490e+00 1.35986172e+00 1.34133043e+00\n",
      " 1.30430106e+00 1.27390776e+00 1.26693302e+00 1.24963291e+00\n",
      " 1.22605047e+00 1.20930857e+00 1.17516276e+00 1.15928254e+00\n",
      " 1.10827155e+00 1.10579309e+00 1.08143299e+00 1.04990833e+00\n",
      " 1.02459246e+00 1.02341005e+00 9.85720533e-01 9.61891596e-01\n",
      " 9.42982519e-01 9.21998780e-01 8.82967456e-01 8.72463017e-01\n",
      " 8.44555158e-01 8.17005973e-01 7.93647028e-01 7.76731812e-01\n",
      " 7.45476639e-01 7.08294780e-01 6.86481937e-01 6.57690591e-01\n",
      " 6.51422345e-01 5.97619248e-01 5.42989137e-01 5.23422102e-01\n",
      " 4.98348925e-01 4.77149980e-01 4.23471134e-01 3.70797065e-01\n",
      " 3.59415774e-01 3.00606079e-01 2.58306813e-01 2.28607677e-01\n",
      " 1.95625143e-01 1.75367642e-01 1.46014513e-01 1.28691052e-01\n",
      " 8.98530433e-02 8.07634505e-02 5.95855167e-02 5.23992103e-02\n",
      " 4.70815083e-02 4.34232852e-02 3.61298754e-02 2.98530038e-02\n",
      " 2.48705681e-02 2.35851227e-02 1.95286451e-02 1.69957990e-02\n",
      " 1.46860880e-02 1.21079527e-02 1.10980698e-02 9.93992518e-03\n",
      " 8.83581804e-03 7.61617126e-03 7.20521993e-03 6.65891856e-03\n",
      " 6.43155722e-03 5.94651053e-03 5.82038515e-03 5.57402635e-03\n",
      " 5.16710101e-03 5.06908641e-03 4.74478140e-03 4.38495356e-03\n",
      " 4.16813362e-03 4.08939712e-03 3.65184440e-03 3.47949909e-03\n",
      " 3.29645439e-03 3.24561828e-03 3.09080106e-03 2.96960479e-03\n",
      " 2.84422802e-03 2.66636480e-03 2.60714860e-03 2.44392856e-03\n",
      " 2.28607000e-03 2.16064731e-03 2.13101155e-03 2.09523859e-03\n",
      " 2.02501915e-03 1.94251359e-03 1.92125743e-03 1.86249722e-03\n",
      " 1.83413055e-03 1.74571568e-03 1.71652971e-03 1.67846122e-03\n",
      " 1.57774025e-03 1.54048222e-03 1.50385920e-03 1.46617952e-03\n",
      " 1.44405312e-03 1.42015405e-03 1.38468954e-03 1.35962379e-03\n",
      " 1.32311004e-03 1.29079373e-03 1.28130524e-03 1.26447037e-03\n",
      " 1.21417661e-03 1.18156807e-03 1.17149410e-03 1.15402245e-03\n",
      " 1.13138434e-03 1.11139484e-03 1.08723355e-03 1.06463934e-03\n",
      " 1.05900574e-03 1.03875135e-03 1.01130509e-03 9.94422771e-04\n",
      " 9.77763215e-04 9.61645653e-04 9.40291655e-04 9.31481056e-04\n",
      " 9.17237684e-04 9.09895749e-04 9.00540358e-04 8.82440489e-04\n",
      " 8.77600637e-04 8.64681170e-04 8.49183888e-04 8.47919815e-04\n",
      " 8.36188645e-04 8.13606259e-04 8.06461647e-04 7.91656685e-04\n",
      " 7.87537605e-04 7.69919355e-04 7.60725888e-04 7.44501528e-04\n",
      " 7.39846716e-04 7.30456784e-04 7.16250308e-04 7.08588571e-04\n",
      " 6.99920974e-04 6.96945088e-04 6.95438919e-04 6.76953446e-04\n",
      " 6.70633304e-04 6.55723181e-04 6.46802028e-04 6.42673428e-04\n",
      " 6.36047590e-04 6.24363606e-04 6.18873107e-04 6.14856934e-04\n",
      " 6.06837331e-04 6.00400230e-04 5.95799374e-04 5.88649844e-04\n",
      " 5.87224146e-04 5.73575076e-04 5.68754828e-04 5.60811273e-04\n",
      " 5.59500017e-04 5.54052382e-04 5.43516854e-04 5.40823128e-04\n",
      " 5.38010822e-04 5.35647654e-04 5.31539068e-04 5.22636787e-04\n",
      " 5.15352779e-04 5.13604532e-04 5.08347723e-04 5.03630612e-04\n",
      " 4.95788463e-04 4.93156317e-04 4.89114666e-04 4.86122788e-04\n",
      " 4.77366778e-04 4.74588437e-04 4.69555699e-04 4.66400719e-04\n",
      " 4.60788245e-04 4.55495115e-04 4.51422460e-04 4.48002291e-04\n",
      " 4.45603325e-04 4.43398202e-04 4.35994025e-04 4.33107565e-04\n",
      " 4.27534264e-04 4.24282181e-04 4.20404387e-04 4.12010008e-04\n",
      " 4.10059373e-04 4.07764398e-04 4.02257420e-04 4.00794722e-04\n",
      " 3.99235784e-04 3.97516295e-04 3.90868507e-04 3.89592037e-04\n",
      " 3.87512351e-04 3.84632269e-04 3.77020784e-04 3.73522364e-04\n",
      " 3.72927493e-04 3.66062967e-04 3.63919240e-04 3.60036860e-04\n",
      " 3.59354962e-04 3.55639701e-04 3.52414461e-04 3.50360789e-04\n",
      " 3.47156264e-04 3.46141020e-04 3.44457453e-04 3.40093148e-04\n",
      " 3.36950601e-04 3.32473051e-04 3.30968644e-04 3.27108307e-04\n",
      " 3.24586611e-04 3.21917088e-04 3.18269984e-04 3.16201073e-04\n",
      " 3.13593124e-04 3.11225940e-04 3.09161930e-04 3.06719205e-04\n",
      " 3.03768834e-04 3.02202086e-04 2.99985008e-04 2.97486014e-04\n",
      " 2.92276651e-04 2.89988534e-04 2.87571679e-04 2.86359582e-04\n",
      " 2.81954368e-04 2.81530935e-04 2.81018894e-04 2.77104065e-04\n",
      " 2.76333412e-04 2.73608658e-04 2.70526215e-04 2.69865961e-04\n",
      " 2.66358817e-04 2.64890279e-04 2.62616183e-04 2.60694146e-04\n",
      " 2.59875665e-04 2.56800500e-04 2.55723048e-04 2.52834586e-04\n",
      " 2.49502489e-04 2.48174272e-04 2.45265661e-04 2.43736249e-04\n",
      " 2.42852404e-04 2.40490233e-04 2.38602052e-04 2.35357983e-04\n",
      " 2.34692255e-04 2.33321361e-04 2.32322432e-04 2.30155091e-04\n",
      " 2.26614611e-04 2.25580995e-04 2.23635578e-04 2.22122875e-04\n",
      " 2.20987783e-04 2.17813711e-04 2.17153065e-04 2.15398017e-04\n",
      " 2.13744459e-04 2.12653408e-04 2.10804164e-04 2.08537320e-04\n",
      " 2.07026353e-04 2.05859286e-04 2.01086440e-04 2.00153275e-04\n",
      " 1.99297855e-04 1.96561414e-04 1.95738365e-04 1.94750089e-04\n",
      " 1.92496281e-04 1.91086155e-04 1.89591196e-04 1.88202819e-04\n",
      " 1.87796557e-04 1.85246885e-04 1.84971438e-04 1.82974150e-04\n",
      " 1.79982109e-04 1.77720238e-04 1.76297165e-04 1.74962580e-04\n",
      " 1.73266720e-04 1.72189163e-04 1.71349523e-04 1.68583511e-04\n",
      " 1.67095673e-04 1.66706061e-04 1.65429352e-04 1.63860504e-04\n",
      " 1.61517146e-04 1.60526333e-04 1.59672788e-04 1.58419226e-04\n",
      " 1.58171467e-04 1.55307888e-04 1.54088931e-04 1.52555335e-04\n",
      " 1.51569923e-04 1.49823879e-04 1.48722874e-04 1.46846461e-04\n",
      " 1.45928812e-04 1.44227513e-04 1.42827932e-04 1.42422769e-04\n",
      " 1.41577243e-04 1.40021808e-04 1.38648701e-04 1.37590029e-04\n",
      " 1.36614973e-04 1.35292333e-04 1.34186086e-04 1.33206958e-04\n",
      " 1.31995798e-04 1.31523648e-04 1.30158084e-04 1.27797622e-04\n",
      " 1.26682466e-04 1.25518742e-04 1.24519029e-04 1.23284655e-04\n",
      " 1.22379661e-04 1.21245636e-04 1.19842088e-04 1.19038137e-04\n",
      " 1.18275595e-04 1.16979033e-04 1.15074766e-04 1.14196537e-04\n",
      " 1.12435868e-04 1.12123236e-04 1.10683549e-04 1.09457730e-04\n",
      " 1.08649877e-04 1.07591438e-04 1.06707342e-04 1.05470747e-04\n",
      " 1.04458549e-04 1.04075350e-04 1.01826185e-04 1.00874041e-04\n",
      " 9.97529951e-05 9.74343900e-05 9.67193363e-05 9.56367751e-05\n",
      " 9.43077499e-05 9.39990215e-05 9.30565845e-05 9.24288979e-05\n",
      " 9.03165224e-05 8.92649220e-05 8.81987869e-05 8.78416478e-05\n",
      " 8.69982863e-05 8.49103057e-05 8.46237219e-05 8.36127361e-05\n",
      " 8.24187036e-05 8.07262356e-05 7.96919998e-05 7.84922378e-05\n",
      " 7.69550140e-05 7.64413830e-05 7.55467501e-05 7.33162343e-05\n",
      " 7.19422311e-05 6.99753625e-05 6.89090799e-05 6.60633110e-05]\n",
      "[8.07989212 7.21778079 5.8904341  5.15080362 4.7302275  4.15403744\n",
      " 3.99383618 3.63447651 3.45939434 3.26295624 3.09979301 2.92855832\n",
      " 2.766262   2.66294305 2.45968911 2.41496127 2.30625234 2.28097716\n",
      " 2.21707954 2.12541285 2.08638457 2.02445998 1.9207608  1.85233115\n",
      " 1.83030956 1.77017587 1.72893062 1.6800222  1.65788071 1.64865859\n",
      " 1.59416565 1.54173326 1.50551477 1.49379866 1.47077623 1.44007072\n",
      " 1.43503925 1.41225207 1.35746827 1.35251838 1.34165387 1.28122493\n",
      " 1.27735732 1.26357575 1.24789799 1.2214984  1.21076324 1.19255159\n",
      " 1.17280037 1.13948023 1.13265809 1.10888312 1.10369793 1.08125672\n",
      " 1.07617833 1.05045515 1.0391252  1.01341374 0.99338571 0.98656491\n",
      " 0.97869527 0.95530038 0.95392633 0.93146179 0.91417809 0.91268754\n",
      " 0.87907744 0.86789281 0.85874722 0.85502889 0.85347226 0.84578537\n",
      " 0.83173277 0.82486657 0.8106771  0.80710072 0.80109976 0.79207146\n",
      " 0.78734789 0.76356117 0.75587216 0.74953435 0.7474362  0.73049122\n",
      " 0.7189019  0.71471786 0.70995011 0.70406988 0.68918971 0.68602996\n",
      " 0.6833013  0.67423361 0.67049552 0.66103874 0.65037413 0.63932224\n",
      " 0.62896356 0.62343266 0.61738815 0.61439185 0.60949894 0.59989343\n",
      " 0.59958939 0.5857487  0.58392853 0.57904407 0.57456792 0.56843072\n",
      " 0.55810519 0.55683323 0.5532243  0.55067637 0.53851161 0.53386454\n",
      " 0.53075318 0.52869247 0.52634416 0.51644044 0.50726647 0.50477602\n",
      " 0.49912914 0.49445388 0.49055227 0.48441464 0.48217132 0.47757698\n",
      " 0.46904246 0.46659346 0.46236693 0.45720133 0.45227449 0.44645786\n",
      " 0.4444343  0.43780789 0.43680809 0.43297653 0.423705   0.4218169\n",
      " 0.41903232 0.41481185 0.41405362 0.40494594 0.40469628 0.39985704\n",
      " 0.39843701 0.39606697 0.3911775  0.38805297 0.38216937 0.38022075\n",
      " 0.37755298 0.37269182 0.37007154 0.36565206 0.36176019 0.35753188\n",
      " 0.35578668 0.35193261 0.35070508 0.34737981 0.34454358 0.3388367\n",
      " 0.33863949 0.33459946 0.3336774  0.33183251 0.32789808 0.32647415\n",
      " 0.32592259 0.32237441 0.31637409 0.31372686 0.31201727 0.30482254\n",
      " 0.30388643 0.30169647 0.2974982  0.29594709 0.29506593 0.29375336\n",
      " 0.28974838 0.28801212 0.28482747 0.2836286  0.2818503  0.27891988\n",
      " 0.27838278 0.27682383 0.27200709 0.27129536 0.26727033 0.26461642\n",
      " 0.26403644 0.26157223 0.2604338  0.259145   0.25689346 0.25412449\n",
      " 0.25350082 0.25163316 0.24835969 0.24733146 0.24404851 0.2425684\n",
      " 0.24150114 0.24019939 0.23905432 0.23738063 0.23686145 0.23276144\n",
      " 0.2310249  0.23038843 0.2290578  0.22742108 0.2253748  0.22484911\n",
      " 0.22187404 0.22056203 0.21903139 0.21817682 0.21658152 0.21564012\n",
      " 0.21493852 0.21210916 0.21100692 0.20996156 0.20782289 0.20643705\n",
      " 0.20559318 0.20371382 0.20289415 0.20206833 0.20116705 0.1992312\n",
      " 0.19808919 0.19706301 0.19419877 0.19348785 0.19239982 0.19174974\n",
      " 0.18997559 0.18813382 0.18812502 0.18766022 0.18529333 0.1833955\n",
      " 0.18312428 0.18128784 0.18025426 0.1789808  0.17783074 0.17610201\n",
      " 0.17564822 0.17375023 0.17182501 0.17086856 0.1707359  0.16986276\n",
      " 0.16832063 0.16798238 0.1659075  0.16535979 0.16471135 0.16332534\n",
      " 0.1621863  0.1615834  0.16110781 0.16014143 0.15954723 0.1587706\n",
      " 0.1582445  0.15665406 0.1556928  0.15531613 0.15376159 0.15328592\n",
      " 0.15302539 0.15042259 0.14977105 0.14915626 0.14852998 0.14758656\n",
      " 0.14660305 0.14491418 0.14446801 0.14386448 0.1424858  0.14144041\n",
      " 0.14098812 0.14021829 0.13897126 0.13843321 0.13773188 0.13763475\n",
      " 0.1368068  0.13573002 0.13505031 0.13403234 0.13302153 0.13271824\n",
      " 0.13215002 0.13107409 0.13082587 0.13016891 0.12954823 0.12807304\n",
      " 0.12747434 0.12649877 0.12578351 0.12500611 0.12449423 0.12391431\n",
      " 0.12339993 0.12295074 0.12201023 0.12125907 0.12112258 0.12035917\n",
      " 0.11989001 0.11861018 0.11815517 0.11804501 0.11720293 0.11634543\n",
      " 0.11519189 0.11413366 0.11380629 0.11270697 0.11193176 0.1119134\n",
      " 0.11147394 0.11131063 0.11007806 0.10948133 0.1093358  0.10859516\n",
      " 0.10797899 0.10781655 0.10735588 0.10664047 0.10614305 0.10574216\n",
      " 0.10432027 0.10399609 0.10350405 0.10329433 0.10260413 0.10237036\n",
      " 0.10134805 0.10108374 0.10033099 0.0999897  0.09942819 0.09860755\n",
      " 0.09834159 0.09761178 0.09748906 0.09652309 0.09593176 0.09534164\n",
      " 0.09516967 0.0949915  0.09457686 0.09318571 0.09291288 0.09258333\n",
      " 0.09239241 0.09166779 0.09128559 0.09093116 0.09031224 0.08960646\n",
      " 0.08925803 0.08909865 0.08898745 0.08851138 0.08828639 0.08795674\n",
      " 0.08723435 0.08683665 0.08616122 0.08572696 0.08562319 0.08517968\n",
      " 0.08470571 0.08397655 0.08321457 0.08309559 0.08281906 0.08222881\n",
      " 0.0818812  0.08124221 0.08087179 0.08070997 0.07987393 0.07951544\n",
      " 0.0793106  0.07885681 0.07842391 0.07764536 0.07753069 0.07729064\n",
      " 0.07714349 0.07658481 0.07640514 0.07634881 0.07562464 0.07522812\n",
      " 0.07491424 0.07391117 0.07375727 0.07355642 0.07313947 0.07262619\n",
      " 0.07203453 0.07187752 0.07153159 0.07134193 0.07094547 0.07043055\n",
      " 0.07027685 0.06983851 0.06974232 0.06934114 0.06862795 0.06840831\n",
      " 0.0681071  0.06787347 0.06715175 0.06681515 0.06659    0.06604506\n",
      " 0.06565256 0.06521825 0.0651241  0.06470717 0.06434369 0.06395644\n",
      " 0.06381314 0.06319551 0.06280488 0.06247117 0.06241157 0.061875\n",
      " 0.06137653 0.06119032 0.06052294 0.06044267 0.05993094 0.05975454\n",
      " 0.05955295 0.05924133 0.05882579 0.05841201 0.05805264 0.05775289\n",
      " 0.0571914  0.05686684 0.05676489 0.05658737 0.05646609 0.05613059\n",
      " 0.05576556 0.05524215 0.05501878 0.05472768 0.05440597 0.05413196\n",
      " 0.05395726 0.05362115 0.05322859 0.05296836 0.05246933 0.05210337\n",
      " 0.05170617 0.05145475 0.05116211 0.05097581 0.05053852 0.05036981\n",
      " 0.04995901 0.0495593  0.04909031 0.04878364 0.04853368 0.04835259\n",
      " 0.04814556 0.0477073  0.04733175 0.04685652 0.04668761 0.04647637\n",
      " 0.04601069 0.0458273  0.04547564 0.04533526 0.04500185 0.04465502\n",
      " 0.0439594  0.0437539  0.04335692 0.04310528 0.04241455 0.0423697\n",
      " 0.04182014 0.04164885 0.04097891 0.04054367 0.03988333 0.03907085\n",
      " 0.03804346 0.03785201]\n"
     ]
    }
   ],
   "source": [
    "transform = lda.ComputeLdaTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # normlize to sqrt(dim)\n",
    "# for i in class_list_new:\n",
    "#     scale = np.sqrt(512) / np.linalg.norm(class_list_new[i], axis=1, keepdims=True)\n",
    "#     class_list_new[i] = scale * class_list_new[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in class_list_new:\n",
    "    class_list_new[i] = class_list_new[i].dot(transform.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normlize to sqrt(dim)\n",
    "for i in class_list_new:\n",
    "    scale = np.sqrt(PLDA_DIM) / np.linalg.norm(class_list_new[i], axis=1, keepdims=True)\n",
    "    class_list_new[i] = scale * class_list_new[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plda_stats = PldaStats(PLDA_DIM)\n",
    "for i in class_list_new:\n",
    "    plda_stats.add_samples(class_list_new[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plda_stats.sort()\n",
    "plda_stats.is_sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5\n",
      "nllr_x: 411.3536651606943\n",
      "nllr_y: -371.394730264205\n",
      "normalized_nllr: 39.95893489648931\n",
      "nllr_m: 412.13621538628524\n",
      "nllr_m_2: 412.13621538628604\n",
      "part1_residual -455.29228631886446\n",
      "part2_mean -412.13621538628524\n",
      "normlized_obj -455.04478465275184\n",
      "2 5\n",
      "nllr_x: 240.15276805056112\n",
      "nllr_y: -458.94205476522495\n",
      "normalized_nllr: -218.7892867146638\n",
      "nllr_m: 246.5813414941023\n",
      "nllr_m_2: 246.5813414941021\n",
      "part1_residual -437.3886274503415\n",
      "part2_mean -246.5813414941023\n",
      "normlized_obj -436.294340594284\n",
      "3 5\n",
      "nllr_x: 237.46553834505684\n",
      "nllr_y: -458.8741951366892\n",
      "normalized_nllr: -221.40865679163232\n",
      "nllr_m: 245.48345787472775\n",
      "nllr_m_2: 245.48345787472786\n",
      "part1_residual -437.3910096499305\n",
      "part2_mean -245.48345787472775\n",
      "normlized_obj -436.2904127286267\n",
      "4 5\n",
      "nllr_x: 237.20065986248494\n",
      "nllr_y: -458.7576755590343\n",
      "normalized_nllr: -221.55701569654934\n",
      "nllr_m: 245.46150954196784\n",
      "nllr_m_2: 245.4615095419679\n",
      "part1_residual -437.3910074823672\n",
      "part2_mean -245.46150954196784\n",
      "normlized_obj -436.2902846989926\n",
      "5 5\n",
      "nllr_x: 237.16177247008068\n",
      "nllr_y: -458.73833011962114\n",
      "normalized_nllr: -221.57655764954046\n",
      "nllr_m: 245.46090097759316\n",
      "nllr_m_2: 245.46090097759335\n",
      "part1_residual -437.3910038566896\n",
      "part2_mean -245.46090097759313\n",
      "normlized_obj -436.2902776039693\n"
     ]
    }
   ],
   "source": [
    "# test_fin_prior\n",
    "plda_estimator = PldaEstimation(plda_stats)\n",
    "plda_paras = plda_estimator.estimate(iteration=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "out = component_dir+'/'+ PLDA_PARA_NAME\n",
    "with open(out, 'wb') as handle:\n",
    "    pickle.dump(plda_paras, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PLDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(component_dir+'/'+ PLDA_PARA_NAME, 'rb') as handle:\n",
    "    plda_paras = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "plda = PLDA(plda_paras[0], plda_paras[1], plda_paras[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(component_dir+'/'+TRAIN_DATA_NAME, 'rb') as handle:\n",
    "    train_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_list = np.zeros([0, 512])\n",
    "for i in train_data:\n",
    "    unlabeled_list = np.append(unlabeled_list, train_data[i], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "major_mean = np.mean(unlabeled_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# out = component_dir+'/major_mean'\n",
    "# with open(out, 'wb') as handle:\n",
    "#     pickle.dump(major_mean, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unlabeled_list = unlabeled_list - major_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unlabeled_list = unlabeled_list.dot(transform.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normlize to sqrt(dim)\n",
    "scale = np.sqrt(PLDA_DIM) / np.linalg.norm(unlabeled_list, axis=1, keepdims=True)\n",
    "unlabeled_list = scale * unlabeled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adaptor = PldaUnsupervisedAdaptor(dim=PLDA_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(unlabeled_list.shape[0]):\n",
    "    adaptor.add_stats(unlabeled_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_plda_paras = adaptor.update_plda(plda_paras[0], plda_paras[1], plda_paras[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "out = component_dir+'/new_plda_paras'\n",
    "with open(out, 'wb') as handle:\n",
    "    pickle.dump(new_plda_paras, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plda = PLDA(new_plda_paras[0], new_plda_paras[1], new_plda_paras[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enroll models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(component_dir+'/'+ENR_DATA_NAME, 'rb') as handle:\n",
    "    enr_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "enr_list = {}\n",
    "num_utt = {}\n",
    "with open('/Lun0/zhiyong/SdSV_2020_deepmine/task2_enrollment/docs/model_enrollment.txt', 'r') as f:\n",
    "    for count, line in enumerate(f):\n",
    "        if count == 0:\n",
    "            continue\n",
    "        info = line[:-1].split(' ')\n",
    "        model_label = info[0]\n",
    "        num_utt[model_label] = len(info)-1\n",
    "        for i in range(1, len(info)):\n",
    "            if model_label not in enr_list.keys():\n",
    "                enr_list[model_label] = enr_data[info[i]]\n",
    "            else:\n",
    "                enr_list[model_label] = np.append(enr_list[model_label], enr_data[info[i]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(component_dir+'/'+EVL_DATA_NAME, 'rb') as handle:\n",
    "    evl_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trail_path = '/Lun0/zhiyong/SdSV_2020_deepmine/task2_enrollment/docs/trials.txt'\n",
    "score_out_path = component_dir+'/'+SCORING_PLDA_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in enr_list:\n",
    "    enr_list[i] = np.mean(enr_list[i], axis=0).squeeze()\n",
    "    enr_list[i] = enr_list[i] - global_mean\n",
    "    enr_list[i] = transform.dot(enr_list[i])\n",
    "    enr_list[i] = (np.sqrt(PLDA_DIM) / np.linalg.norm(enr_list[i])) * enr_list[i]\n",
    "    num = num_utt[i]\n",
    "    enr_list[i] = plda.transform_ivector(enr_list[i], num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in evl_data:\n",
    "    evl_data[i] = evl_data[i].squeeze()\n",
    "    evl_data[i] = evl_data[i] - global_mean\n",
    "    evl_data[i] = transform.dot(evl_data[i])\n",
    "    evl_data[i] = (np.sqrt(PLDA_DIM) / np.linalg.norm(evl_data[i])) * evl_data[i]\n",
    "    evl_data[i] = plda.transform_ivector(evl_data[i], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-id evaluation-file-id\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n",
      "135000\n",
      "140000\n",
      "145000\n",
      "150000\n",
      "155000\n",
      "160000\n",
      "165000\n",
      "170000\n",
      "175000\n",
      "180000\n",
      "185000\n",
      "190000\n",
      "195000\n",
      "200000\n",
      "205000\n",
      "210000\n",
      "215000\n",
      "220000\n",
      "225000\n",
      "230000\n",
      "235000\n",
      "240000\n",
      "245000\n",
      "250000\n",
      "255000\n",
      "260000\n",
      "265000\n",
      "270000\n",
      "275000\n",
      "280000\n",
      "285000\n",
      "290000\n",
      "295000\n",
      "300000\n",
      "305000\n",
      "310000\n",
      "315000\n",
      "320000\n",
      "325000\n",
      "330000\n",
      "335000\n",
      "340000\n",
      "345000\n",
      "350000\n",
      "355000\n",
      "360000\n",
      "365000\n",
      "370000\n",
      "375000\n",
      "380000\n",
      "385000\n",
      "390000\n",
      "395000\n",
      "400000\n",
      "405000\n",
      "410000\n",
      "415000\n",
      "420000\n",
      "425000\n",
      "430000\n",
      "435000\n",
      "440000\n",
      "445000\n",
      "450000\n",
      "455000\n",
      "460000\n",
      "465000\n",
      "470000\n",
      "475000\n",
      "480000\n",
      "485000\n",
      "490000\n",
      "495000\n",
      "500000\n",
      "505000\n",
      "510000\n",
      "515000\n",
      "520000\n",
      "525000\n",
      "530000\n",
      "535000\n",
      "540000\n",
      "545000\n",
      "550000\n",
      "555000\n",
      "560000\n",
      "565000\n",
      "570000\n",
      "575000\n",
      "580000\n",
      "585000\n",
      "590000\n",
      "595000\n",
      "600000\n",
      "605000\n",
      "610000\n",
      "615000\n",
      "620000\n",
      "625000\n",
      "630000\n",
      "635000\n",
      "640000\n",
      "645000\n",
      "650000\n",
      "655000\n",
      "660000\n",
      "665000\n",
      "670000\n",
      "675000\n",
      "680000\n",
      "685000\n",
      "690000\n",
      "695000\n",
      "700000\n",
      "705000\n",
      "710000\n",
      "715000\n",
      "720000\n",
      "725000\n",
      "730000\n",
      "735000\n",
      "740000\n",
      "745000\n",
      "750000\n",
      "755000\n",
      "760000\n",
      "765000\n",
      "770000\n",
      "775000\n",
      "780000\n",
      "785000\n",
      "790000\n",
      "795000\n",
      "800000\n",
      "805000\n",
      "810000\n",
      "815000\n",
      "820000\n",
      "825000\n",
      "830000\n",
      "835000\n",
      "840000\n",
      "845000\n",
      "850000\n",
      "855000\n",
      "860000\n",
      "865000\n",
      "870000\n",
      "875000\n",
      "880000\n",
      "885000\n",
      "890000\n",
      "895000\n",
      "900000\n",
      "905000\n",
      "910000\n",
      "915000\n",
      "920000\n",
      "925000\n",
      "930000\n",
      "935000\n",
      "940000\n",
      "945000\n",
      "950000\n",
      "955000\n",
      "960000\n",
      "965000\n",
      "970000\n",
      "975000\n",
      "980000\n",
      "985000\n",
      "990000\n",
      "995000\n",
      "1000000\n",
      "1005000\n",
      "1010000\n",
      "1015000\n",
      "1020000\n",
      "1025000\n",
      "1030000\n",
      "1035000\n",
      "1040000\n",
      "1045000\n",
      "1050000\n",
      "1055000\n",
      "1060000\n",
      "1065000\n",
      "1070000\n",
      "1075000\n",
      "1080000\n",
      "1085000\n",
      "1090000\n",
      "1095000\n",
      "1100000\n",
      "1105000\n",
      "1110000\n",
      "1115000\n",
      "1120000\n",
      "1125000\n",
      "1130000\n",
      "1135000\n",
      "1140000\n",
      "1145000\n",
      "1150000\n",
      "1155000\n",
      "1160000\n",
      "1165000\n",
      "1170000\n",
      "1175000\n",
      "1180000\n",
      "1185000\n",
      "1190000\n",
      "1195000\n",
      "1200000\n",
      "1205000\n",
      "1210000\n",
      "1215000\n",
      "1220000\n",
      "1225000\n",
      "1230000\n",
      "1235000\n",
      "1240000\n",
      "1245000\n",
      "1250000\n",
      "1255000\n",
      "1260000\n",
      "1265000\n",
      "1270000\n",
      "1275000\n",
      "1280000\n",
      "1285000\n",
      "1290000\n",
      "1295000\n",
      "1300000\n",
      "1305000\n",
      "1310000\n",
      "1315000\n",
      "1320000\n",
      "1325000\n",
      "1330000\n",
      "1335000\n",
      "1340000\n",
      "1345000\n",
      "1350000\n",
      "1355000\n",
      "1360000\n",
      "1365000\n",
      "1370000\n",
      "1375000\n",
      "1380000\n",
      "1385000\n",
      "1390000\n",
      "1395000\n",
      "1400000\n",
      "1405000\n",
      "1410000\n",
      "1415000\n",
      "1420000\n",
      "1425000\n",
      "1430000\n",
      "1435000\n",
      "1440000\n",
      "1445000\n",
      "1450000\n",
      "1455000\n",
      "1460000\n",
      "1465000\n",
      "1470000\n",
      "1475000\n",
      "1480000\n",
      "1485000\n",
      "1490000\n",
      "1495000\n",
      "1500000\n",
      "1505000\n",
      "1510000\n",
      "1515000\n",
      "1520000\n",
      "1525000\n",
      "1530000\n",
      "1535000\n",
      "1540000\n",
      "1545000\n",
      "1550000\n",
      "1555000\n",
      "1560000\n",
      "1565000\n",
      "1570000\n",
      "1575000\n",
      "1580000\n",
      "1585000\n",
      "1590000\n",
      "1595000\n",
      "1600000\n",
      "1605000\n",
      "1610000\n",
      "1615000\n",
      "1620000\n",
      "1625000\n",
      "1630000\n",
      "1635000\n",
      "1640000\n",
      "1645000\n",
      "1650000\n",
      "1655000\n",
      "1660000\n",
      "1665000\n",
      "1670000\n",
      "1675000\n",
      "1680000\n",
      "1685000\n",
      "1690000\n",
      "1695000\n",
      "1700000\n",
      "1705000\n",
      "1710000\n",
      "1715000\n",
      "1720000\n",
      "1725000\n",
      "1730000\n",
      "1735000\n",
      "1740000\n",
      "1745000\n",
      "1750000\n",
      "1755000\n",
      "1760000\n",
      "1765000\n",
      "1770000\n",
      "1775000\n",
      "1780000\n",
      "1785000\n",
      "1790000\n",
      "1795000\n",
      "1800000\n",
      "1805000\n",
      "1810000\n",
      "1815000\n",
      "1820000\n",
      "1825000\n",
      "1830000\n",
      "1835000\n",
      "1840000\n",
      "1845000\n",
      "1850000\n",
      "1855000\n",
      "1860000\n",
      "1865000\n",
      "1870000\n",
      "1875000\n",
      "1880000\n",
      "1885000\n",
      "1890000\n",
      "1895000\n",
      "1900000\n",
      "1905000\n",
      "1910000\n",
      "1915000\n",
      "1920000\n",
      "1925000\n",
      "1930000\n",
      "1935000\n",
      "1940000\n",
      "1945000\n",
      "1950000\n",
      "1955000\n",
      "1960000\n",
      "1965000\n",
      "1970000\n",
      "1975000\n",
      "1980000\n",
      "1985000\n",
      "1990000\n",
      "1995000\n",
      "2000000\n",
      "2005000\n",
      "2010000\n",
      "2015000\n",
      "2020000\n",
      "2025000\n",
      "2030000\n",
      "2035000\n",
      "2040000\n",
      "2045000\n",
      "2050000\n",
      "2055000\n",
      "2060000\n",
      "2065000\n",
      "2070000\n",
      "2075000\n",
      "2080000\n",
      "2085000\n",
      "2090000\n",
      "2095000\n",
      "2100000\n",
      "2105000\n",
      "2110000\n",
      "2115000\n",
      "2120000\n",
      "2125000\n",
      "2130000\n",
      "2135000\n",
      "2140000\n",
      "2145000\n",
      "2150000\n",
      "2155000\n",
      "2160000\n",
      "2165000\n",
      "2170000\n",
      "2175000\n",
      "2180000\n",
      "2185000\n",
      "2190000\n",
      "2195000\n",
      "2200000\n",
      "2205000\n",
      "2210000\n",
      "2215000\n",
      "2220000\n",
      "2225000\n",
      "2230000\n",
      "2235000\n",
      "2240000\n",
      "2245000\n",
      "2250000\n",
      "2255000\n",
      "2260000\n",
      "2265000\n",
      "2270000\n",
      "2275000\n",
      "2280000\n",
      "2285000\n",
      "2290000\n",
      "2295000\n",
      "2300000\n",
      "2305000\n",
      "2310000\n",
      "2315000\n",
      "2320000\n",
      "2325000\n",
      "2330000\n",
      "2335000\n",
      "2340000\n",
      "2345000\n",
      "2350000\n",
      "2355000\n",
      "2360000\n",
      "2365000\n",
      "2370000\n",
      "2375000\n",
      "2380000\n",
      "2385000\n",
      "2390000\n",
      "2395000\n",
      "2400000\n",
      "2405000\n",
      "2410000\n",
      "2415000\n",
      "2420000\n",
      "2425000\n",
      "2430000\n",
      "2435000\n",
      "2440000\n",
      "2445000\n",
      "2450000\n",
      "2455000\n",
      "2460000\n",
      "2465000\n",
      "2470000\n",
      "2475000\n",
      "2480000\n",
      "2485000\n",
      "2490000\n",
      "2495000\n",
      "2500000\n",
      "2505000\n",
      "2510000\n",
      "2515000\n",
      "2520000\n",
      "2525000\n",
      "2530000\n",
      "2535000\n",
      "2540000\n",
      "2545000\n",
      "2550000\n",
      "2555000\n",
      "2560000\n",
      "2565000\n",
      "2570000\n",
      "2575000\n",
      "2580000\n",
      "2585000\n",
      "2590000\n",
      "2595000\n",
      "2600000\n",
      "2605000\n",
      "2610000\n",
      "2615000\n",
      "2620000\n",
      "2625000\n",
      "2630000\n",
      "2635000\n",
      "2640000\n",
      "2645000\n",
      "2650000\n",
      "2655000\n",
      "2660000\n",
      "2665000\n",
      "2670000\n",
      "2675000\n",
      "2680000\n",
      "2685000\n",
      "2690000\n",
      "2695000\n",
      "2700000\n",
      "2705000\n",
      "2710000\n",
      "2715000\n",
      "2720000\n",
      "2725000\n",
      "2730000\n",
      "2735000\n",
      "2740000\n",
      "2745000\n",
      "2750000\n",
      "2755000\n",
      "2760000\n",
      "2765000\n",
      "2770000\n",
      "2775000\n",
      "2780000\n",
      "2785000\n",
      "2790000\n",
      "2795000\n",
      "2800000\n",
      "2805000\n",
      "2810000\n",
      "2815000\n",
      "2820000\n",
      "2825000\n",
      "2830000\n",
      "2835000\n",
      "2840000\n",
      "2845000\n",
      "2850000\n",
      "2855000\n",
      "2860000\n",
      "2865000\n",
      "2870000\n",
      "2875000\n",
      "2880000\n",
      "2885000\n",
      "2890000\n",
      "2895000\n",
      "2900000\n",
      "2905000\n",
      "2910000\n",
      "2915000\n",
      "2920000\n",
      "2925000\n",
      "2930000\n",
      "2935000\n",
      "2940000\n",
      "2945000\n",
      "2950000\n",
      "2955000\n",
      "2960000\n",
      "2965000\n",
      "2970000\n",
      "2975000\n",
      "2980000\n",
      "2985000\n",
      "2990000\n",
      "2995000\n",
      "3000000\n",
      "3005000\n",
      "3010000\n",
      "3015000\n",
      "3020000\n",
      "3025000\n",
      "3030000\n",
      "3035000\n",
      "3040000\n",
      "3045000\n",
      "3050000\n",
      "3055000\n",
      "3060000\n",
      "3065000\n",
      "3070000\n",
      "3075000\n",
      "3080000\n",
      "3085000\n",
      "3090000\n",
      "3095000\n",
      "3100000\n",
      "3105000\n",
      "3110000\n",
      "3115000\n",
      "3120000\n",
      "3125000\n",
      "3130000\n",
      "3135000\n",
      "3140000\n",
      "3145000\n",
      "3150000\n",
      "3155000\n",
      "3160000\n",
      "3165000\n",
      "3170000\n",
      "3175000\n",
      "3180000\n",
      "3185000\n",
      "3190000\n",
      "3195000\n",
      "3200000\n",
      "3205000\n",
      "3210000\n",
      "3215000\n",
      "3220000\n",
      "3225000\n",
      "3230000\n",
      "3235000\n",
      "3240000\n",
      "3245000\n",
      "3250000\n",
      "3255000\n",
      "3260000\n",
      "3265000\n",
      "3270000\n",
      "3275000\n",
      "3280000\n",
      "3285000\n",
      "3290000\n",
      "3295000\n",
      "3300000\n",
      "3305000\n",
      "3310000\n",
      "3315000\n",
      "3320000\n",
      "3325000\n",
      "3330000\n",
      "3335000\n",
      "3340000\n",
      "3345000\n",
      "3350000\n",
      "3355000\n",
      "3360000\n",
      "3365000\n",
      "3370000\n",
      "3375000\n",
      "3380000\n",
      "3385000\n",
      "3390000\n",
      "3395000\n",
      "3400000\n",
      "3405000\n",
      "3410000\n",
      "3415000\n",
      "3420000\n",
      "3425000\n",
      "3430000\n",
      "3435000\n",
      "3440000\n",
      "3445000\n",
      "3450000\n",
      "3455000\n",
      "3460000\n",
      "3465000\n",
      "3470000\n",
      "3475000\n",
      "3480000\n",
      "3485000\n",
      "3490000\n",
      "3495000\n",
      "3500000\n",
      "3505000\n",
      "3510000\n",
      "3515000\n",
      "3520000\n",
      "3525000\n",
      "3530000\n",
      "3535000\n",
      "3540000\n",
      "3545000\n",
      "3550000\n",
      "3555000\n",
      "3560000\n",
      "3565000\n",
      "3570000\n",
      "3575000\n",
      "3580000\n",
      "3585000\n",
      "3590000\n",
      "3595000\n",
      "3600000\n",
      "3605000\n",
      "3610000\n",
      "3615000\n",
      "3620000\n",
      "3625000\n",
      "3630000\n",
      "3635000\n",
      "3640000\n",
      "3645000\n",
      "3650000\n",
      "3655000\n",
      "3660000\n",
      "3665000\n",
      "3670000\n",
      "3675000\n",
      "3680000\n",
      "3685000\n",
      "3690000\n",
      "3695000\n",
      "3700000\n",
      "3705000\n",
      "3710000\n",
      "3715000\n",
      "3720000\n",
      "3725000\n",
      "3730000\n",
      "3735000\n",
      "3740000\n",
      "3745000\n",
      "3750000\n",
      "3755000\n",
      "3760000\n",
      "3765000\n",
      "3770000\n",
      "3775000\n",
      "3780000\n",
      "3785000\n",
      "3790000\n",
      "3795000\n",
      "3800000\n",
      "3805000\n",
      "3810000\n",
      "3815000\n",
      "3820000\n",
      "3825000\n",
      "3830000\n",
      "3835000\n",
      "3840000\n",
      "3845000\n",
      "3850000\n",
      "3855000\n",
      "3860000\n",
      "3865000\n",
      "3870000\n",
      "3875000\n",
      "3880000\n",
      "3885000\n",
      "3890000\n",
      "3895000\n",
      "3900000\n",
      "3905000\n",
      "3910000\n",
      "3915000\n",
      "3920000\n",
      "3925000\n",
      "3930000\n",
      "3935000\n",
      "3940000\n",
      "3945000\n",
      "3950000\n",
      "3955000\n",
      "3960000\n",
      "3965000\n",
      "3970000\n",
      "3975000\n",
      "3980000\n",
      "3985000\n",
      "3990000\n",
      "3995000\n",
      "4000000\n",
      "4005000\n",
      "4010000\n",
      "4015000\n",
      "4020000\n",
      "4025000\n",
      "4030000\n",
      "4035000\n",
      "4040000\n",
      "4045000\n",
      "4050000\n",
      "4055000\n",
      "4060000\n",
      "4065000\n",
      "4070000\n",
      "4075000\n",
      "4080000\n",
      "4085000\n",
      "4090000\n",
      "4095000\n",
      "4100000\n",
      "4105000\n",
      "4110000\n",
      "4115000\n",
      "4120000\n",
      "4125000\n",
      "4130000\n",
      "4135000\n",
      "4140000\n",
      "4145000\n",
      "4150000\n",
      "4155000\n",
      "4160000\n",
      "4165000\n",
      "4170000\n",
      "4175000\n",
      "4180000\n",
      "4185000\n",
      "4190000\n",
      "4195000\n",
      "4200000\n",
      "4205000\n",
      "4210000\n",
      "4215000\n",
      "4220000\n",
      "4225000\n",
      "4230000\n",
      "4235000\n",
      "4240000\n",
      "4245000\n",
      "4250000\n",
      "4255000\n",
      "4260000\n",
      "4265000\n",
      "4270000\n",
      "4275000\n",
      "4280000\n",
      "4285000\n",
      "4290000\n",
      "4295000\n",
      "4300000\n",
      "4305000\n",
      "4310000\n",
      "4315000\n",
      "4320000\n",
      "4325000\n",
      "4330000\n",
      "4335000\n",
      "4340000\n",
      "4345000\n",
      "4350000\n",
      "4355000\n",
      "4360000\n",
      "4365000\n",
      "4370000\n",
      "4375000\n",
      "4380000\n",
      "4385000\n",
      "4390000\n",
      "4395000\n",
      "4400000\n",
      "4405000\n",
      "4410000\n",
      "4415000\n",
      "4420000\n",
      "4425000\n",
      "4430000\n",
      "4435000\n",
      "4440000\n",
      "4445000\n",
      "4450000\n",
      "4455000\n",
      "4460000\n",
      "4465000\n",
      "4470000\n",
      "4475000\n",
      "4480000\n",
      "4485000\n",
      "4490000\n",
      "4495000\n",
      "4500000\n",
      "4505000\n",
      "4510000\n",
      "4515000\n",
      "4520000\n",
      "4525000\n",
      "4530000\n",
      "4535000\n",
      "4540000\n",
      "4545000\n",
      "4550000\n",
      "4555000\n",
      "4560000\n",
      "4565000\n",
      "4570000\n",
      "4575000\n",
      "4580000\n",
      "4585000\n",
      "4590000\n",
      "4595000\n",
      "4600000\n",
      "4605000\n",
      "4610000\n",
      "4615000\n",
      "4620000\n",
      "4625000\n",
      "4630000\n",
      "4635000\n",
      "4640000\n",
      "4645000\n",
      "4650000\n",
      "4655000\n",
      "4660000\n",
      "4665000\n",
      "4670000\n",
      "4675000\n",
      "4680000\n",
      "4685000\n",
      "4690000\n",
      "4695000\n",
      "4700000\n",
      "4705000\n",
      "4710000\n",
      "4715000\n",
      "4720000\n",
      "4725000\n",
      "4730000\n",
      "4735000\n",
      "4740000\n",
      "4745000\n",
      "4750000\n",
      "4755000\n",
      "4760000\n",
      "4765000\n",
      "4770000\n",
      "4775000\n",
      "4780000\n",
      "4785000\n",
      "4790000\n",
      "4795000\n",
      "4800000\n",
      "4805000\n",
      "4810000\n",
      "4815000\n",
      "4820000\n",
      "4825000\n",
      "4830000\n",
      "4835000\n",
      "4840000\n",
      "4845000\n",
      "4850000\n",
      "4855000\n",
      "4860000\n",
      "4865000\n",
      "4870000\n",
      "4875000\n",
      "4880000\n",
      "4885000\n",
      "4890000\n",
      "4895000\n",
      "4900000\n",
      "4905000\n",
      "4910000\n",
      "4915000\n",
      "4920000\n",
      "4925000\n",
      "4930000\n",
      "4935000\n",
      "4940000\n",
      "4945000\n",
      "4950000\n",
      "4955000\n",
      "4960000\n",
      "4965000\n",
      "4970000\n",
      "4975000\n",
      "4980000\n",
      "4985000\n",
      "4990000\n",
      "4995000\n",
      "5000000\n",
      "5005000\n",
      "5010000\n",
      "5015000\n",
      "5020000\n",
      "5025000\n",
      "5030000\n",
      "5035000\n",
      "5040000\n",
      "5045000\n",
      "5050000\n",
      "5055000\n",
      "5060000\n",
      "5065000\n",
      "5070000\n",
      "5075000\n",
      "5080000\n",
      "5085000\n",
      "5090000\n",
      "5095000\n",
      "5100000\n",
      "5105000\n",
      "5110000\n",
      "5115000\n",
      "5120000\n",
      "5125000\n",
      "5130000\n",
      "5135000\n",
      "5140000\n",
      "5145000\n",
      "5150000\n",
      "5155000\n",
      "5160000\n",
      "5165000\n",
      "5170000\n",
      "5175000\n",
      "5180000\n",
      "5185000\n",
      "5190000\n",
      "5195000\n",
      "5200000\n",
      "5205000\n",
      "5210000\n",
      "5215000\n",
      "5220000\n",
      "5225000\n",
      "5230000\n",
      "5235000\n",
      "5240000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5245000\n",
      "5250000\n",
      "5255000\n",
      "5260000\n",
      "5265000\n",
      "5270000\n",
      "5275000\n",
      "5280000\n",
      "5285000\n",
      "5290000\n",
      "5295000\n",
      "5300000\n",
      "5305000\n",
      "5310000\n",
      "5315000\n",
      "5320000\n",
      "5325000\n",
      "5330000\n",
      "5335000\n",
      "5340000\n",
      "5345000\n",
      "5350000\n",
      "5355000\n",
      "5360000\n",
      "5365000\n",
      "5370000\n",
      "5375000\n",
      "5380000\n",
      "5385000\n",
      "5390000\n",
      "5395000\n",
      "5400000\n",
      "5405000\n",
      "5410000\n",
      "5415000\n",
      "5420000\n",
      "5425000\n",
      "5430000\n",
      "5435000\n",
      "5440000\n",
      "5445000\n",
      "5450000\n",
      "5455000\n",
      "5460000\n",
      "5465000\n",
      "5470000\n",
      "5475000\n",
      "5480000\n",
      "5485000\n",
      "5490000\n",
      "5495000\n",
      "5500000\n",
      "5505000\n",
      "5510000\n",
      "5515000\n",
      "5520000\n",
      "5525000\n",
      "5530000\n",
      "5535000\n",
      "5540000\n",
      "5545000\n",
      "5550000\n",
      "5555000\n",
      "5560000\n",
      "5565000\n",
      "5570000\n",
      "5575000\n",
      "5580000\n",
      "5585000\n",
      "5590000\n",
      "5595000\n",
      "5600000\n",
      "5605000\n",
      "5610000\n",
      "5615000\n",
      "5620000\n",
      "5625000\n",
      "5630000\n",
      "5635000\n",
      "5640000\n",
      "5645000\n",
      "5650000\n",
      "5655000\n",
      "5660000\n",
      "5665000\n",
      "5670000\n",
      "5675000\n",
      "5680000\n",
      "5685000\n",
      "5690000\n",
      "5695000\n",
      "5700000\n",
      "5705000\n",
      "5710000\n",
      "5715000\n",
      "5720000\n",
      "5725000\n",
      "5730000\n",
      "5735000\n",
      "5740000\n",
      "5745000\n",
      "5750000\n",
      "5755000\n",
      "5760000\n",
      "5765000\n",
      "5770000\n",
      "5775000\n",
      "5780000\n",
      "5785000\n",
      "5790000\n",
      "5795000\n",
      "5800000\n",
      "5805000\n",
      "5810000\n",
      "5815000\n",
      "5820000\n",
      "5825000\n",
      "5830000\n",
      "5835000\n",
      "5840000\n",
      "5845000\n",
      "5850000\n",
      "5855000\n",
      "5860000\n",
      "5865000\n",
      "5870000\n",
      "5875000\n",
      "5880000\n",
      "5885000\n",
      "5890000\n",
      "5895000\n",
      "5900000\n",
      "5905000\n",
      "5910000\n",
      "5915000\n",
      "5920000\n",
      "5925000\n",
      "5930000\n",
      "5935000\n",
      "5940000\n",
      "5945000\n",
      "5950000\n",
      "5955000\n",
      "5960000\n",
      "5965000\n",
      "5970000\n",
      "5975000\n",
      "5980000\n",
      "5985000\n",
      "5990000\n",
      "5995000\n",
      "6000000\n",
      "6005000\n",
      "6010000\n",
      "6015000\n",
      "6020000\n",
      "6025000\n",
      "6030000\n",
      "6035000\n",
      "6040000\n",
      "6045000\n",
      "6050000\n",
      "6055000\n",
      "6060000\n",
      "6065000\n",
      "6070000\n",
      "6075000\n",
      "6080000\n",
      "6085000\n",
      "6090000\n",
      "6095000\n",
      "6100000\n",
      "6105000\n",
      "6110000\n",
      "6115000\n",
      "6120000\n",
      "6125000\n",
      "6130000\n",
      "6135000\n",
      "6140000\n",
      "6145000\n",
      "6150000\n",
      "6155000\n",
      "6160000\n",
      "6165000\n",
      "6170000\n",
      "6175000\n",
      "6180000\n",
      "6185000\n",
      "6190000\n",
      "6195000\n",
      "6200000\n",
      "6205000\n",
      "6210000\n",
      "6215000\n",
      "6220000\n",
      "6225000\n",
      "6230000\n",
      "6235000\n",
      "6240000\n",
      "6245000\n",
      "6250000\n",
      "6255000\n",
      "6260000\n",
      "6265000\n",
      "6270000\n",
      "6275000\n",
      "6280000\n",
      "6285000\n",
      "6290000\n",
      "6295000\n",
      "6300000\n",
      "6305000\n",
      "6310000\n",
      "6315000\n",
      "6320000\n",
      "6325000\n",
      "6330000\n",
      "6335000\n",
      "6340000\n",
      "6345000\n",
      "6350000\n",
      "6355000\n",
      "6360000\n",
      "6365000\n",
      "6370000\n",
      "6375000\n",
      "6380000\n",
      "6385000\n",
      "6390000\n",
      "6395000\n",
      "6400000\n",
      "6405000\n",
      "6410000\n",
      "6415000\n",
      "6420000\n",
      "6425000\n",
      "6430000\n",
      "6435000\n",
      "6440000\n",
      "6445000\n",
      "6450000\n",
      "6455000\n",
      "6460000\n",
      "6465000\n",
      "6470000\n",
      "6475000\n",
      "6480000\n",
      "6485000\n",
      "6490000\n",
      "6495000\n",
      "6500000\n",
      "6505000\n",
      "6510000\n",
      "6515000\n",
      "6520000\n",
      "6525000\n",
      "6530000\n",
      "6535000\n",
      "6540000\n",
      "6545000\n",
      "6550000\n",
      "6555000\n",
      "6560000\n",
      "6565000\n",
      "6570000\n",
      "6575000\n",
      "6580000\n",
      "6585000\n",
      "6590000\n",
      "6595000\n",
      "6600000\n",
      "6605000\n",
      "6610000\n",
      "6615000\n",
      "6620000\n",
      "6625000\n",
      "6630000\n",
      "6635000\n",
      "6640000\n",
      "6645000\n",
      "6650000\n",
      "6655000\n",
      "6660000\n",
      "6665000\n",
      "6670000\n",
      "6675000\n",
      "6680000\n",
      "6685000\n",
      "6690000\n",
      "6695000\n",
      "6700000\n",
      "6705000\n",
      "6710000\n",
      "6715000\n",
      "6720000\n",
      "6725000\n",
      "6730000\n",
      "6735000\n",
      "6740000\n",
      "6745000\n",
      "6750000\n",
      "6755000\n",
      "6760000\n",
      "6765000\n",
      "6770000\n",
      "6775000\n",
      "6780000\n",
      "6785000\n",
      "6790000\n",
      "6795000\n",
      "6800000\n",
      "6805000\n",
      "6810000\n",
      "6815000\n",
      "6820000\n",
      "6825000\n",
      "6830000\n",
      "6835000\n",
      "6840000\n",
      "6845000\n",
      "6850000\n",
      "6855000\n",
      "6860000\n",
      "6865000\n",
      "6870000\n",
      "6875000\n",
      "6880000\n",
      "6885000\n",
      "6890000\n",
      "6895000\n",
      "6900000\n",
      "6905000\n",
      "6910000\n",
      "6915000\n",
      "6920000\n",
      "6925000\n",
      "6930000\n",
      "6935000\n",
      "6940000\n",
      "6945000\n",
      "6950000\n",
      "6955000\n",
      "6960000\n",
      "6965000\n",
      "6970000\n",
      "6975000\n",
      "6980000\n",
      "6985000\n",
      "6990000\n",
      "6995000\n",
      "7000000\n",
      "7005000\n",
      "7010000\n",
      "7015000\n",
      "7020000\n",
      "7025000\n",
      "7030000\n",
      "7035000\n",
      "7040000\n",
      "7045000\n",
      "7050000\n",
      "7055000\n",
      "7060000\n",
      "7065000\n",
      "7070000\n",
      "7075000\n",
      "7080000\n",
      "7085000\n",
      "7090000\n",
      "7095000\n",
      "7100000\n",
      "7105000\n",
      "7110000\n",
      "7115000\n",
      "7120000\n",
      "7125000\n",
      "7130000\n",
      "7135000\n",
      "7140000\n",
      "7145000\n",
      "7150000\n",
      "7155000\n",
      "7160000\n",
      "7165000\n",
      "7170000\n",
      "7175000\n",
      "7180000\n",
      "7185000\n",
      "7190000\n",
      "7195000\n",
      "7200000\n",
      "7205000\n",
      "7210000\n",
      "7215000\n",
      "7220000\n",
      "7225000\n",
      "7230000\n",
      "7235000\n",
      "7240000\n",
      "7245000\n",
      "7250000\n",
      "7255000\n",
      "7260000\n",
      "7265000\n",
      "7270000\n",
      "7275000\n",
      "7280000\n",
      "7285000\n",
      "7290000\n",
      "7295000\n",
      "7300000\n",
      "7305000\n",
      "7310000\n",
      "7315000\n",
      "7320000\n",
      "7325000\n",
      "7330000\n",
      "7335000\n",
      "7340000\n",
      "7345000\n",
      "7350000\n",
      "7355000\n",
      "7360000\n",
      "7365000\n",
      "7370000\n",
      "7375000\n",
      "7380000\n",
      "7385000\n",
      "7390000\n",
      "7395000\n",
      "7400000\n",
      "7405000\n",
      "7410000\n",
      "7415000\n",
      "7420000\n",
      "7425000\n",
      "7430000\n",
      "7435000\n",
      "7440000\n",
      "7445000\n",
      "7450000\n",
      "7455000\n",
      "7460000\n",
      "7465000\n",
      "7470000\n",
      "7475000\n",
      "7480000\n",
      "7485000\n",
      "7490000\n",
      "7495000\n",
      "7500000\n",
      "7505000\n",
      "7510000\n",
      "7515000\n",
      "7520000\n",
      "7525000\n",
      "7530000\n",
      "7535000\n",
      "7540000\n",
      "7545000\n",
      "7550000\n",
      "7555000\n",
      "7560000\n",
      "7565000\n",
      "7570000\n",
      "7575000\n",
      "7580000\n",
      "7585000\n",
      "7590000\n",
      "7595000\n",
      "7600000\n",
      "7605000\n",
      "7610000\n",
      "7615000\n",
      "7620000\n",
      "7625000\n",
      "7630000\n",
      "7635000\n",
      "7640000\n",
      "7645000\n",
      "7650000\n",
      "7655000\n",
      "7660000\n",
      "7665000\n",
      "7670000\n",
      "7675000\n",
      "7680000\n",
      "7685000\n",
      "7690000\n",
      "7695000\n",
      "7700000\n",
      "7705000\n",
      "7710000\n",
      "7715000\n",
      "7720000\n",
      "7725000\n",
      "7730000\n",
      "7735000\n",
      "7740000\n",
      "7745000\n",
      "7750000\n",
      "7755000\n",
      "7760000\n",
      "7765000\n",
      "7770000\n",
      "7775000\n",
      "7780000\n",
      "7785000\n",
      "7790000\n",
      "7795000\n",
      "7800000\n",
      "7805000\n",
      "7810000\n",
      "7815000\n",
      "7820000\n",
      "7825000\n",
      "7830000\n",
      "7835000\n",
      "7840000\n",
      "7845000\n",
      "7850000\n",
      "7855000\n",
      "7860000\n",
      "7865000\n",
      "7870000\n",
      "7875000\n",
      "7880000\n",
      "7885000\n",
      "7890000\n",
      "7895000\n",
      "7900000\n",
      "7905000\n",
      "7910000\n",
      "7915000\n",
      "7920000\n",
      "7925000\n",
      "7930000\n",
      "7935000\n",
      "7940000\n",
      "7945000\n",
      "7950000\n",
      "7955000\n",
      "7960000\n",
      "7965000\n",
      "7970000\n",
      "7975000\n",
      "7980000\n",
      "7985000\n",
      "7990000\n",
      "7995000\n",
      "8000000\n",
      "8005000\n",
      "8010000\n",
      "8015000\n",
      "8020000\n",
      "8025000\n",
      "8030000\n",
      "8035000\n",
      "8040000\n",
      "8045000\n",
      "8050000\n",
      "8055000\n",
      "8060000\n",
      "8065000\n",
      "8070000\n",
      "8075000\n",
      "8080000\n",
      "8085000\n",
      "8090000\n",
      "8095000\n",
      "8100000\n",
      "8105000\n",
      "8110000\n",
      "8115000\n",
      "8120000\n",
      "8125000\n",
      "8130000\n",
      "8135000\n",
      "8140000\n",
      "8145000\n",
      "8150000\n",
      "8155000\n",
      "8160000\n",
      "8165000\n",
      "8170000\n",
      "8175000\n",
      "8180000\n",
      "8185000\n",
      "8190000\n",
      "8195000\n",
      "8200000\n",
      "8205000\n",
      "8210000\n",
      "8215000\n",
      "8220000\n",
      "8225000\n",
      "8230000\n",
      "8235000\n",
      "8240000\n",
      "8245000\n",
      "8250000\n",
      "8255000\n",
      "8260000\n",
      "8265000\n",
      "8270000\n",
      "8275000\n",
      "8280000\n",
      "8285000\n",
      "8290000\n",
      "8295000\n",
      "8300000\n",
      "8305000\n",
      "8310000\n",
      "8315000\n",
      "8320000\n",
      "8325000\n",
      "8330000\n",
      "8335000\n",
      "8340000\n",
      "8345000\n",
      "8350000\n",
      "8355000\n",
      "8360000\n",
      "8365000\n",
      "8370000\n",
      "8375000\n",
      "8380000\n",
      "8385000\n",
      "8390000\n",
      "8395000\n",
      "8400000\n",
      "8405000\n",
      "8410000\n",
      "8415000\n",
      "8420000\n",
      "8425000\n",
      "8430000\n",
      "8435000\n",
      "8440000\n",
      "8445000\n",
      "8450000\n",
      "8455000\n",
      "8460000\n",
      "8465000\n",
      "8470000\n",
      "8475000\n",
      "8480000\n",
      "8485000\n",
      "8490000\n",
      "8495000\n",
      "8500000\n",
      "8505000\n",
      "8510000\n",
      "8515000\n",
      "8520000\n",
      "8525000\n",
      "8530000\n",
      "8535000\n",
      "8540000\n",
      "8545000\n",
      "8550000\n",
      "8555000\n",
      "8560000\n",
      "8565000\n",
      "8570000\n",
      "8575000\n",
      "8580000\n",
      "8585000\n",
      "8590000\n",
      "8595000\n",
      "8600000\n",
      "8605000\n",
      "8610000\n",
      "8615000\n",
      "8620000\n",
      "8625000\n",
      "8630000\n",
      "8635000\n",
      "8640000\n",
      "8645000\n",
      "8650000\n",
      "8655000\n",
      "8660000\n",
      "8665000\n",
      "8670000\n",
      "8675000\n",
      "8680000\n",
      "8685000\n",
      "8690000\n",
      "8695000\n",
      "8700000\n",
      "8705000\n",
      "8710000\n",
      "8715000\n",
      "8720000\n",
      "8725000\n",
      "8730000\n",
      "8735000\n",
      "8740000\n",
      "8745000\n",
      "8750000\n",
      "8755000\n",
      "8760000\n",
      "8765000\n",
      "8770000\n",
      "8775000\n",
      "8780000\n",
      "8785000\n",
      "8790000\n",
      "8795000\n",
      "8800000\n",
      "8805000\n",
      "8810000\n",
      "8815000\n",
      "8820000\n",
      "8825000\n",
      "8830000\n",
      "8835000\n",
      "8840000\n",
      "8845000\n",
      "8850000\n",
      "8855000\n",
      "8860000\n",
      "8865000\n",
      "8870000\n",
      "8875000\n",
      "8880000\n",
      "8885000\n",
      "8890000\n",
      "8895000\n",
      "8900000\n",
      "8905000\n",
      "8910000\n",
      "8915000\n",
      "8920000\n",
      "8925000\n",
      "8930000\n",
      "8935000\n",
      "8940000\n",
      "8945000\n",
      "8950000\n",
      "8955000\n",
      "8960000\n",
      "8965000\n",
      "8970000\n",
      "8975000\n",
      "8980000\n",
      "8985000\n",
      "8990000\n",
      "8995000\n",
      "9000000\n",
      "9005000\n",
      "9010000\n",
      "9015000\n",
      "9020000\n",
      "9025000\n",
      "9030000\n",
      "9035000\n",
      "9040000\n",
      "9045000\n",
      "9050000\n",
      "9055000\n",
      "9060000\n",
      "9065000\n",
      "9070000\n",
      "9075000\n",
      "9080000\n",
      "9085000\n",
      "9090000\n",
      "9095000\n",
      "9100000\n",
      "9105000\n",
      "9110000\n",
      "9115000\n",
      "9120000\n",
      "9125000\n",
      "9130000\n",
      "9135000\n",
      "9140000\n",
      "9145000\n",
      "9150000\n",
      "9155000\n",
      "9160000\n",
      "9165000\n",
      "9170000\n",
      "9175000\n",
      "9180000\n",
      "9185000\n",
      "9190000\n",
      "9195000\n",
      "9200000\n",
      "9205000\n",
      "9210000\n",
      "9215000\n",
      "9220000\n",
      "9225000\n",
      "9230000\n",
      "9235000\n",
      "9240000\n",
      "9245000\n",
      "9250000\n",
      "9255000\n",
      "9260000\n",
      "9265000\n",
      "9270000\n",
      "9275000\n",
      "9280000\n",
      "9285000\n",
      "9290000\n",
      "9295000\n",
      "9300000\n",
      "9305000\n",
      "9310000\n",
      "9315000\n",
      "9320000\n",
      "9325000\n",
      "9330000\n",
      "9335000\n",
      "9340000\n",
      "9345000\n",
      "9350000\n",
      "9355000\n",
      "9360000\n",
      "9365000\n",
      "9370000\n",
      "9375000\n",
      "9380000\n",
      "9385000\n",
      "9390000\n",
      "9395000\n",
      "9400000\n",
      "9405000\n",
      "9410000\n",
      "9415000\n",
      "9420000\n",
      "9425000\n",
      "9430000\n",
      "9435000\n",
      "9440000\n",
      "9445000\n",
      "9450000\n",
      "9455000\n",
      "9460000\n",
      "9465000\n",
      "9470000\n",
      "9475000\n",
      "9480000\n",
      "9485000\n",
      "9490000\n",
      "9495000\n",
      "9500000\n",
      "9505000\n",
      "9510000\n",
      "9515000\n",
      "9520000\n",
      "9525000\n",
      "9530000\n",
      "9535000\n",
      "9540000\n",
      "9545000\n",
      "9550000\n",
      "9555000\n",
      "9560000\n",
      "9565000\n",
      "9570000\n",
      "9575000\n",
      "9580000\n",
      "9585000\n",
      "9590000\n",
      "9595000\n",
      "9600000\n",
      "9605000\n",
      "9610000\n",
      "9615000\n",
      "9620000\n",
      "9625000\n",
      "9630000\n",
      "9635000\n",
      "9640000\n",
      "9645000\n",
      "9650000\n",
      "9655000\n",
      "9660000\n",
      "9665000\n",
      "9670000\n",
      "9675000\n",
      "9680000\n",
      "9685000\n",
      "9690000\n",
      "9695000\n",
      "9700000\n",
      "9705000\n",
      "9710000\n",
      "9715000\n",
      "9720000\n",
      "9725000\n",
      "9730000\n",
      "9735000\n",
      "9740000\n",
      "9745000\n",
      "9750000\n",
      "9755000\n",
      "9760000\n",
      "9765000\n",
      "9770000\n",
      "9775000\n",
      "9780000\n",
      "9785000\n",
      "9790000\n",
      "9795000\n",
      "9800000\n",
      "9805000\n",
      "9810000\n",
      "9815000\n",
      "9820000\n",
      "9825000\n",
      "9830000\n",
      "9835000\n",
      "9840000\n",
      "9845000\n",
      "9850000\n",
      "9855000\n",
      "9860000\n",
      "9865000\n",
      "9870000\n",
      "9875000\n",
      "9880000\n",
      "9885000\n",
      "9890000\n",
      "9895000\n",
      "9900000\n",
      "9905000\n",
      "9910000\n",
      "9915000\n",
      "9920000\n",
      "9925000\n",
      "9930000\n",
      "9935000\n",
      "9940000\n",
      "9945000\n",
      "9950000\n",
      "9955000\n",
      "9960000\n",
      "9965000\n",
      "9970000\n",
      "9975000\n",
      "9980000\n",
      "9985000\n",
      "9990000\n",
      "9995000\n",
      "10000000\n",
      "10005000\n",
      "10010000\n",
      "10015000\n",
      "10020000\n",
      "10025000\n",
      "10030000\n",
      "10035000\n",
      "10040000\n",
      "10045000\n",
      "10050000\n",
      "10055000\n",
      "10060000\n",
      "10065000\n",
      "10070000\n",
      "10075000\n",
      "10080000\n",
      "10085000\n",
      "10090000\n",
      "10095000\n",
      "10100000\n",
      "10105000\n",
      "10110000\n",
      "10115000\n",
      "10120000\n",
      "10125000\n",
      "10130000\n",
      "10135000\n",
      "10140000\n",
      "10145000\n",
      "10150000\n",
      "10155000\n",
      "10160000\n",
      "10165000\n",
      "10170000\n",
      "10175000\n",
      "10180000\n",
      "10185000\n",
      "10190000\n",
      "10195000\n",
      "10200000\n",
      "10205000\n",
      "10210000\n",
      "10215000\n",
      "10220000\n",
      "10225000\n",
      "10230000\n",
      "10235000\n",
      "10240000\n",
      "10245000\n",
      "10250000\n",
      "10255000\n",
      "10260000\n",
      "10265000\n",
      "10270000\n",
      "10275000\n",
      "10280000\n",
      "10285000\n",
      "10290000\n",
      "10295000\n",
      "10300000\n",
      "10305000\n",
      "10310000\n",
      "10315000\n",
      "10320000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10325000\n",
      "10330000\n",
      "10335000\n",
      "10340000\n",
      "10345000\n",
      "10350000\n",
      "10355000\n",
      "10360000\n",
      "10365000\n",
      "10370000\n",
      "10375000\n",
      "10380000\n",
      "10385000\n",
      "10390000\n",
      "10395000\n",
      "10400000\n",
      "10405000\n",
      "10410000\n",
      "10415000\n",
      "10420000\n",
      "10425000\n",
      "10430000\n",
      "10435000\n",
      "10440000\n",
      "10445000\n",
      "10450000\n",
      "10455000\n",
      "10460000\n",
      "10465000\n",
      "10470000\n",
      "10475000\n",
      "10480000\n",
      "10485000\n",
      "10490000\n",
      "10495000\n",
      "10500000\n",
      "10505000\n",
      "10510000\n",
      "10515000\n",
      "10520000\n",
      "10525000\n",
      "10530000\n",
      "10535000\n",
      "10540000\n",
      "10545000\n",
      "10550000\n",
      "10555000\n",
      "10560000\n",
      "10565000\n",
      "10570000\n",
      "10575000\n",
      "10580000\n",
      "10585000\n",
      "10590000\n",
      "10595000\n",
      "10600000\n",
      "10605000\n",
      "10610000\n",
      "10615000\n",
      "10620000\n",
      "10625000\n",
      "10630000\n",
      "10635000\n",
      "10640000\n",
      "10645000\n",
      "10650000\n",
      "10655000\n",
      "10660000\n",
      "10665000\n",
      "10670000\n",
      "10675000\n",
      "10680000\n",
      "10685000\n",
      "10690000\n",
      "10695000\n",
      "10700000\n",
      "10705000\n",
      "10710000\n",
      "10715000\n",
      "10720000\n",
      "10725000\n",
      "10730000\n",
      "10735000\n",
      "10740000\n",
      "10745000\n",
      "10750000\n",
      "10755000\n",
      "10760000\n",
      "10765000\n",
      "10770000\n",
      "10775000\n",
      "10780000\n",
      "10785000\n",
      "10790000\n",
      "10795000\n",
      "10800000\n",
      "10805000\n",
      "10810000\n",
      "10815000\n",
      "10820000\n",
      "10825000\n",
      "10830000\n",
      "10835000\n",
      "10840000\n",
      "10845000\n",
      "10850000\n",
      "10855000\n",
      "10860000\n",
      "10865000\n",
      "10870000\n",
      "10875000\n",
      "10880000\n",
      "10885000\n",
      "10890000\n",
      "10895000\n",
      "10900000\n",
      "10905000\n",
      "10910000\n",
      "10915000\n",
      "10920000\n",
      "10925000\n",
      "10930000\n",
      "10935000\n",
      "10940000\n",
      "10945000\n",
      "10950000\n",
      "10955000\n",
      "10960000\n",
      "10965000\n",
      "10970000\n",
      "10975000\n",
      "10980000\n",
      "10985000\n",
      "10990000\n",
      "10995000\n",
      "11000000\n",
      "11005000\n",
      "11010000\n",
      "11015000\n",
      "11020000\n",
      "11025000\n",
      "11030000\n",
      "11035000\n",
      "11040000\n",
      "11045000\n",
      "11050000\n",
      "11055000\n",
      "11060000\n",
      "11065000\n",
      "11070000\n",
      "11075000\n",
      "11080000\n",
      "11085000\n",
      "11090000\n",
      "11095000\n",
      "11100000\n",
      "11105000\n",
      "11110000\n",
      "11115000\n",
      "11120000\n",
      "11125000\n",
      "11130000\n",
      "11135000\n",
      "11140000\n",
      "11145000\n",
      "11150000\n",
      "11155000\n",
      "11160000\n",
      "11165000\n",
      "11170000\n",
      "11175000\n",
      "11180000\n",
      "11185000\n",
      "11190000\n",
      "11195000\n",
      "11200000\n",
      "11205000\n",
      "11210000\n",
      "11215000\n",
      "11220000\n",
      "11225000\n",
      "11230000\n",
      "11235000\n",
      "11240000\n",
      "11245000\n",
      "11250000\n",
      "11255000\n",
      "11260000\n",
      "11265000\n",
      "11270000\n",
      "11275000\n",
      "11280000\n",
      "11285000\n",
      "11290000\n",
      "11295000\n",
      "11300000\n",
      "11305000\n",
      "11310000\n",
      "11315000\n",
      "11320000\n",
      "11325000\n",
      "11330000\n",
      "11335000\n",
      "11340000\n",
      "11345000\n",
      "11350000\n",
      "11355000\n",
      "11360000\n",
      "11365000\n",
      "11370000\n",
      "11375000\n",
      "11380000\n",
      "11385000\n",
      "11390000\n",
      "11395000\n",
      "11400000\n",
      "11405000\n",
      "11410000\n",
      "11415000\n",
      "11420000\n",
      "11425000\n",
      "11430000\n",
      "11435000\n",
      "11440000\n",
      "11445000\n",
      "11450000\n",
      "11455000\n",
      "11460000\n",
      "11465000\n",
      "11470000\n",
      "11475000\n",
      "11480000\n",
      "11485000\n",
      "11490000\n",
      "11495000\n",
      "11500000\n",
      "11505000\n",
      "11510000\n",
      "11515000\n",
      "11520000\n",
      "11525000\n",
      "11530000\n",
      "11535000\n",
      "11540000\n",
      "11545000\n",
      "11550000\n",
      "11555000\n",
      "11560000\n",
      "11565000\n",
      "11570000\n",
      "11575000\n",
      "11580000\n",
      "11585000\n",
      "11590000\n",
      "11595000\n",
      "11600000\n",
      "11605000\n",
      "11610000\n",
      "11615000\n",
      "11620000\n",
      "11625000\n",
      "11630000\n",
      "11635000\n",
      "11640000\n",
      "11645000\n",
      "11650000\n",
      "11655000\n",
      "11660000\n",
      "11665000\n",
      "11670000\n",
      "11675000\n",
      "11680000\n",
      "11685000\n",
      "11690000\n",
      "11695000\n",
      "11700000\n",
      "11705000\n",
      "11710000\n",
      "11715000\n",
      "11720000\n",
      "11725000\n",
      "11730000\n",
      "11735000\n",
      "11740000\n",
      "11745000\n",
      "11750000\n",
      "11755000\n",
      "11760000\n",
      "11765000\n",
      "11770000\n",
      "11775000\n",
      "11780000\n",
      "11785000\n",
      "11790000\n",
      "11795000\n",
      "11800000\n",
      "11805000\n",
      "11810000\n",
      "11815000\n",
      "11820000\n",
      "11825000\n",
      "11830000\n",
      "11835000\n",
      "11840000\n",
      "11845000\n",
      "11850000\n",
      "11855000\n",
      "11860000\n",
      "11865000\n",
      "11870000\n",
      "11875000\n",
      "11880000\n",
      "11885000\n",
      "11890000\n",
      "11895000\n",
      "11900000\n",
      "11905000\n",
      "11910000\n",
      "11915000\n",
      "11920000\n",
      "11925000\n",
      "11930000\n",
      "11935000\n",
      "11940000\n",
      "11945000\n",
      "11950000\n",
      "11955000\n",
      "11960000\n",
      "11965000\n",
      "11970000\n",
      "11975000\n",
      "11980000\n",
      "11985000\n",
      "11990000\n",
      "11995000\n",
      "12000000\n",
      "12005000\n",
      "12010000\n",
      "12015000\n",
      "12020000\n",
      "12025000\n",
      "12030000\n",
      "12035000\n",
      "12040000\n",
      "12045000\n",
      "12050000\n",
      "12055000\n",
      "12060000\n",
      "12065000\n",
      "12070000\n",
      "12075000\n",
      "12080000\n",
      "12085000\n",
      "12090000\n",
      "12095000\n",
      "12100000\n",
      "12105000\n",
      "12110000\n",
      "12115000\n",
      "12120000\n",
      "12125000\n",
      "12130000\n",
      "12135000\n",
      "12140000\n",
      "12145000\n",
      "12150000\n",
      "12155000\n",
      "12160000\n",
      "12165000\n",
      "12170000\n",
      "12175000\n",
      "12180000\n",
      "12185000\n",
      "12190000\n",
      "12195000\n",
      "12200000\n",
      "12205000\n",
      "12210000\n",
      "12215000\n",
      "12220000\n",
      "12225000\n",
      "12230000\n",
      "12235000\n",
      "12240000\n",
      "12245000\n",
      "12250000\n",
      "12255000\n",
      "12260000\n",
      "12265000\n",
      "12270000\n",
      "12275000\n",
      "12280000\n",
      "12285000\n",
      "12290000\n",
      "12295000\n",
      "12300000\n",
      "12305000\n",
      "12310000\n",
      "12315000\n",
      "12320000\n",
      "12325000\n",
      "12330000\n",
      "12335000\n",
      "12340000\n",
      "12345000\n",
      "12350000\n",
      "12355000\n",
      "12360000\n",
      "12365000\n",
      "12370000\n",
      "12375000\n",
      "12380000\n",
      "12385000\n",
      "12390000\n",
      "12395000\n",
      "12400000\n",
      "12405000\n",
      "12410000\n",
      "12415000\n",
      "12420000\n",
      "12425000\n",
      "12430000\n",
      "12435000\n",
      "12440000\n",
      "12445000\n",
      "12450000\n",
      "12455000\n",
      "12460000\n",
      "12465000\n",
      "12470000\n",
      "12475000\n",
      "12480000\n",
      "12485000\n",
      "12490000\n",
      "12495000\n",
      "12500000\n",
      "12505000\n",
      "12510000\n",
      "12515000\n",
      "12520000\n",
      "12525000\n",
      "12530000\n",
      "12535000\n",
      "12540000\n",
      "12545000\n",
      "12550000\n",
      "12555000\n",
      "12560000\n",
      "12565000\n",
      "12570000\n",
      "12575000\n",
      "12580000\n",
      "12585000\n",
      "12590000\n",
      "12595000\n",
      "12600000\n",
      "12605000\n",
      "12610000\n",
      "12615000\n",
      "12620000\n",
      "12625000\n",
      "12630000\n",
      "12635000\n",
      "12640000\n",
      "12645000\n",
      "12650000\n",
      "12655000\n",
      "12660000\n",
      "12665000\n",
      "12670000\n",
      "12675000\n",
      "12680000\n",
      "12685000\n",
      "12690000\n",
      "12695000\n",
      "12700000\n",
      "12705000\n",
      "12710000\n",
      "12715000\n",
      "12720000\n",
      "12725000\n",
      "12730000\n",
      "12735000\n",
      "12740000\n",
      "12745000\n",
      "12750000\n",
      "12755000\n",
      "12760000\n",
      "12765000\n",
      "12770000\n",
      "12775000\n",
      "12780000\n",
      "12785000\n",
      "12790000\n",
      "12795000\n",
      "12800000\n",
      "12805000\n",
      "12810000\n",
      "12815000\n",
      "12820000\n",
      "12825000\n",
      "12830000\n",
      "12835000\n",
      "12840000\n",
      "12845000\n",
      "12850000\n",
      "12855000\n",
      "12860000\n",
      "12865000\n",
      "12870000\n",
      "12875000\n",
      "12880000\n",
      "12885000\n",
      "12890000\n",
      "12895000\n",
      "12900000\n",
      "12905000\n",
      "12910000\n",
      "12915000\n",
      "12920000\n",
      "12925000\n",
      "12930000\n",
      "12935000\n",
      "12940000\n",
      "12945000\n",
      "12950000\n",
      "12955000\n",
      "12960000\n",
      "12965000\n",
      "12970000\n",
      "12975000\n",
      "12980000\n",
      "12985000\n",
      "12990000\n",
      "12995000\n",
      "13000000\n",
      "13005000\n",
      "13010000\n",
      "13015000\n",
      "13020000\n",
      "13025000\n",
      "13030000\n",
      "13035000\n",
      "13040000\n",
      "13045000\n",
      "13050000\n",
      "13055000\n",
      "13060000\n",
      "13065000\n",
      "13070000\n",
      "13075000\n",
      "13080000\n",
      "13085000\n",
      "13090000\n",
      "13095000\n",
      "13100000\n",
      "13105000\n",
      "13110000\n",
      "13115000\n",
      "13120000\n",
      "13125000\n",
      "13130000\n",
      "13135000\n",
      "13140000\n",
      "13145000\n",
      "13150000\n",
      "13155000\n",
      "13160000\n",
      "13165000\n",
      "13170000\n",
      "13175000\n",
      "13180000\n",
      "13185000\n",
      "13190000\n",
      "13195000\n"
     ]
    }
   ],
   "source": [
    "with open(score_out_path, 'w') as of:\n",
    "    with open(trail_path, 'r') as f:\n",
    "        for count, line in enumerate(f):\n",
    "            line = line[:-1]\n",
    "            if count == 0:\n",
    "                print(line)\n",
    "                continue\n",
    "            enroll_emb = enr_list[line.split(' ')[0]].squeeze()\n",
    "            num = num_utt[line.split(' ')[0]]\n",
    "            test_emb = evl_data[line.split(' ')[1]].squeeze()\n",
    "\n",
    "            cosine = plda.log_likelihood_ratio(enroll_emb, num, test_emb)\n",
    "            \n",
    "            of.write(str(cosine)+'\\n')\n",
    "            \n",
    "            if (count+1) % 5000 == 0:\n",
    "                print(count+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13198024"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scoring adapt plda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trail_path = '/Lun0/zhiyong/SdSV_2020_deepmine/task2_enrollment/docs/trials.txt'\n",
    "score_out_path = component_dir+'/'+SCORING_PLDA_NAME+'adapt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in enr_list:\n",
    "    enr_list[i] = np.mean(enr_list[i], axis=0).squeeze()\n",
    "    enr_list[i] = enr_list[i] - major_mean\n",
    "    enr_list[i] = transform.dot(enr_list[i])\n",
    "    enr_list[i] = (np.sqrt(PLDA_DIM) / np.linalg.norm(enr_list[i])) * enr_list[i]\n",
    "    num = num_utt[i]\n",
    "    enr_list[i] = plda.transform_ivector(enr_list[i], num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in evl_data:\n",
    "    evl_data[i] = evl_data[i].squeeze()\n",
    "    evl_data[i] = evl_data[i] - major_mean\n",
    "    evl_data[i] = transform.dot(evl_data[i])\n",
    "    evl_data[i] = (np.sqrt(PLDA_DIM) / np.linalg.norm(evl_data[i])) * evl_data[i]\n",
    "    evl_data[i] = plda.transform_ivector(evl_data[i], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-id evaluation-file-id\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n",
      "20.0\n",
      "21.0\n",
      "22.0\n",
      "23.0\n",
      "24.0\n",
      "25.0\n",
      "26.0\n",
      "27.0\n",
      "28.0\n",
      "29.0\n",
      "30.0\n",
      "31.0\n",
      "32.0\n",
      "33.0\n",
      "34.0\n",
      "35.0\n",
      "36.0\n",
      "37.0\n",
      "38.0\n",
      "39.0\n",
      "40.0\n",
      "41.0\n",
      "42.0\n",
      "43.0\n",
      "44.0\n",
      "45.0\n",
      "46.0\n",
      "47.0\n",
      "48.0\n",
      "49.0\n",
      "50.0\n",
      "51.0\n",
      "52.0\n",
      "53.0\n",
      "54.0\n",
      "55.0\n",
      "56.0\n",
      "57.0\n",
      "58.0\n",
      "59.0\n",
      "60.0\n",
      "61.0\n",
      "62.0\n",
      "63.0\n",
      "64.0\n",
      "65.0\n",
      "66.0\n",
      "67.0\n",
      "68.0\n",
      "69.0\n",
      "70.0\n",
      "71.0\n",
      "72.0\n",
      "73.0\n",
      "74.0\n",
      "75.0\n",
      "76.0\n",
      "77.0\n",
      "78.0\n",
      "79.0\n",
      "80.0\n",
      "81.0\n",
      "82.0\n",
      "83.0\n",
      "84.0\n",
      "85.0\n",
      "86.0\n",
      "87.0\n",
      "88.0\n",
      "89.0\n",
      "90.0\n",
      "91.0\n",
      "92.0\n",
      "93.0\n",
      "94.0\n",
      "95.0\n",
      "96.0\n",
      "97.0\n",
      "98.0\n",
      "99.0\n",
      "100.0\n",
      "101.0\n",
      "102.0\n",
      "103.0\n",
      "104.0\n",
      "105.0\n",
      "106.0\n",
      "107.0\n",
      "108.0\n",
      "109.0\n",
      "110.0\n",
      "111.0\n",
      "112.0\n",
      "113.0\n",
      "114.0\n",
      "115.0\n",
      "116.0\n",
      "117.0\n",
      "118.0\n",
      "119.0\n",
      "120.0\n",
      "121.0\n",
      "122.0\n",
      "123.0\n",
      "124.0\n",
      "125.0\n",
      "126.0\n",
      "127.0\n",
      "128.0\n",
      "129.0\n",
      "130.0\n",
      "131.0\n"
     ]
    }
   ],
   "source": [
    "with open(score_out_path, 'w') as of:\n",
    "    with open(trail_path, 'r') as f:\n",
    "        for count, line in enumerate(f):\n",
    "            line = line[:-1]\n",
    "            if count == 0:\n",
    "                print(line)\n",
    "                continue\n",
    "            enroll_emb = enr_list[line.split(' ')[0]].squeeze()\n",
    "            num = num_utt[line.split(' ')[0]]\n",
    "            test_emb = evl_data[line.split(' ')[1]].squeeze()\n",
    "\n",
    "            cosine = plda.log_likelihood_ratio(enroll_emb, num, test_emb)\n",
    "            \n",
    "            of.write(str(cosine)+'\\n')\n",
    "            \n",
    "            if (count+1) % 100000 == 0:\n",
    "                print((count+1) // 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13198024"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cosine Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trail_path = '/Lun0/zhiyong/SdSV_2020_deepmine/task2_enrollment/docs/trials.txt'\n",
    "score_out_path = component_dir+'/'+SCORING_COSINE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in enr_list:\n",
    "    enr_list[i] = np.mean(enr_list[i], axis=0).squeeze()\n",
    "    enr_list[i] = (1.0 / np.linalg.norm(enr_list[i])) * enr_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in evl_data:\n",
    "    evl_data[i] = evl_data[i].squeeze()\n",
    "    evl_data[i] = (1.0 / np.linalg.norm(evl_data[i])) * evl_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-id evaluation-file-id\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n"
     ]
    }
   ],
   "source": [
    "with open(score_out_path, 'w') as of:\n",
    "    with open(trail_path, 'r') as f:\n",
    "        for count, line in enumerate(f):\n",
    "            line = line[:-1]\n",
    "            if count == 0:\n",
    "                print(line)\n",
    "                continue\n",
    "            enroll_emb = enr_list[line.split(' ')[0]].squeeze()\n",
    "            test_emb = evl_data[line.split(' ')[1]].squeeze()\n",
    "\n",
    "            cosine = np.dot(enroll_emb, test_emb)\n",
    "            \n",
    "            of.write(str(cosine)+'\\n')\n",
    "            \n",
    "            if (count+1) % 100000 == 0:\n",
    "                print((count+1) // 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from my_scorer import scoring\n",
    "from calibrate_scores import calibrating\n",
    "from apply_calibration import applying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set\tEER[%]\tmin_C\tact_C\n",
      "OUT\t01.98\t0.216\t0.303\n",
      "Starting point for CLLR is 0.320928\n",
      "Converged linear model with loss 0.08190182800917162\n",
      "\n",
      "Set\tEER[%]\tmin_C\tact_C\n",
      "OUT\t01.98\t0.216\t0.258\n"
     ]
    }
   ],
   "source": [
    "score_file = component_dir+'/'+SCORING_PLDA_NAME\n",
    "key_file = '/Lun0/zhiyong/dataset/vox1_kaldi_test/trials'\n",
    "calib_score_file = score_file+'_calib'\n",
    "linear_model_pth = component_dir+'/'+'calib.pth'\n",
    "\n",
    "_ = scoring(score_file, key_file)\n",
    "calibrating(linear_model_pth, 50, key_file, [score_file])\n",
    "applying(linear_model_pth, [score_file], calib_score_file)\n",
    "_ = scoring(calib_score_file, key_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set\tEER[%]\tmin_C\tact_C\n",
      "OUT\t02.18\t0.231\t1.000\n",
      "Starting point for CLLR is 0.842448\n",
      "Converged linear model with loss 0.08916190600896338\n",
      "\n",
      "Set\tEER[%]\tmin_C\tact_C\n",
      "OUT\t02.18\t0.231\t0.260\n"
     ]
    }
   ],
   "source": [
    "score_file = component_dir+'/'+SCORING_COSINE_NAME\n",
    "key_file = '/Lun0/zhiyong/dataset/vox1_kaldi_test/trials'\n",
    "calib_score_file = score_file+'_calib'\n",
    "linear_model_pth = component_dir+'/'+'calib.pth'\n",
    "\n",
    "_ = scoring(score_file, key_file)\n",
    "calibrating(linear_model_pth, 50, key_file, [score_file])\n",
    "applying(linear_model_pth, [score_file], calib_score_file)\n",
    "_ = scoring(calib_score_file, key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from score_norm_1 import score_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/dell/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "ori_score = component_dir+'/'+SCORING_PLDA_NAME\n",
    "norm_score = component_dir+'/'+SCORING_PLDA_NAME+'_norm'\n",
    "score_norm(ori_score, norm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set\tEER[%]\tmin_C\tact_C\n",
      "OUT\t01.88\t0.312\t1.000\n",
      "Starting point for CLLR is 0.538934\n",
      "Converged linear model with loss 0.07954009941343457\n",
      "\n",
      "Set\tEER[%]\tmin_C\tact_C\n",
      "OUT\t01.88\t0.312\t0.334\n"
     ]
    }
   ],
   "source": [
    "score_file = component_dir+'/'+SCORING_PLDA_NAME+'_norm'\n",
    "key_file = '/Lun0/zhiyong/dataset/vox1_kaldi_test/trials'\n",
    "calib_score_file = score_file+'_calib'\n",
    "linear_model_pth = component_dir+'/'+'calib.pth'\n",
    "\n",
    "_ = scoring(score_file, key_file)\n",
    "calibrating(linear_model_pth, 50, key_file, [score_file])\n",
    "applying(linear_model_pth, [score_file], calib_score_file)\n",
    "_ = scoring(calib_score_file, key_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/dell/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "ori_score = component_dir+'/'+SCORING_COSINE_NAME\n",
    "norm_score = component_dir+'/'+SCORING_COSINE_NAME+'_norm'\n",
    "score_norm(ori_score, norm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set\tEER[%]\tmin_C\tact_C\n",
      "OUT\t01.81\t0.311\t1.000\n",
      "Starting point for CLLR is 0.536303\n",
      "Converged linear model with loss 0.07717334626909308\n",
      "\n",
      "Set\tEER[%]\tmin_C\tact_C\n",
      "OUT\t01.81\t0.311\t0.331\n"
     ]
    }
   ],
   "source": [
    "score_file = component_dir+'/'+SCORING_COSINE_NAME+'_norm'\n",
    "key_file = '/Lun0/zhiyong/dataset/vox1_kaldi_test/trials'\n",
    "calib_score_file = score_file+'_calib'\n",
    "linear_model_pth = component_dir+'/'+'calib.pth'\n",
    "\n",
    "_ = scoring(score_file, key_file)\n",
    "calibrating(linear_model_pth, 50, key_file, [score_file])\n",
    "applying(linear_model_pth, [score_file], calib_score_file)\n",
    "_ = scoring(calib_score_file, key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point for CLLR is 0.581688\n",
      "Converged linear model with loss 0.074283602014203\n",
      "\n",
      "Set\tEER[%]\tmin_C\tact_C\n",
      "OUT\t01.74\t0.204\t0.221\n"
     ]
    }
   ],
   "source": [
    "score_file_1 = component_dir+'/'+SCORING_PLDA_NAME\n",
    "score_file_2 = component_dir+'/'+SCORING_COSINE_NAME\n",
    "key_file = '/Lun0/zhiyong/dataset/vox1_kaldi_test/trials'\n",
    "calib_score_file = component_dir+'/'+'fuse'\n",
    "linear_model_pth = component_dir+'/'+'calib.pth'\n",
    "\n",
    "# _ = scoring(score_file, key_file)\n",
    "calibrating(linear_model_pth, 50, key_file, [score_file_1, score_file_2])\n",
    "applying(linear_model_pth, [score_file_1, score_file_2], calib_score_file)\n",
    "_ = scoring(calib_score_file, key_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point for CLLR is 0.537618\n",
      "Converged linear model with loss 0.0694378887070766\n",
      "\n",
      "Set\tEER[%]\tmin_C\tact_C\n",
      "OUT\t01.61\t0.266\t0.291\n"
     ]
    }
   ],
   "source": [
    "score_file_1 = component_dir+'/'+SCORING_PLDA_NAME+'_norm'\n",
    "score_file_2 = component_dir+'/'+SCORING_COSINE_NAME+'_norm'\n",
    "key_file = '/Lun0/zhiyong/dataset/vox1_kaldi_test/trials'\n",
    "calib_score_file = component_dir+'/'+'fuse_norm'\n",
    "linear_model_pth = component_dir+'/'+'calib.pth'\n",
    "\n",
    "# _ = scoring(score_file, key_file)\n",
    "calibrating(linear_model_pth, 50, key_file, [score_file_1, score_file_2])\n",
    "applying(linear_model_pth, [score_file_1, score_file_2], calib_score_file)\n",
    "_ = scoring(calib_score_file, key_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
