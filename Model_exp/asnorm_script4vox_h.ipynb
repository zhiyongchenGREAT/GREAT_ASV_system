{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import torch\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "from scipy.io import wavfile\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import random\n",
    "# from asnorm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cohort set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/workspace/LOGS_OUTPUT/server9_nvme1/ASV_LOGS_202102/train_dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from DatasetLoader import loadWAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpeakerNetModel = importlib.import_module('models.'+'EPACA-TDNN').__getattribute__('MainModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPACA-TDNN.py, Embedding size is 192, Channels 1024, Spec_aug False.\n"
     ]
    }
   ],
   "source": [
    "# EPACA-TDNN\n",
    "S = SpeakerNetModel(n_mels=40, nOut=192, spec_aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/workspace/LOGS_OUTPUT/server9_nvme1/ASV_LOGS_202102/train_logs_201120/multi_gpu_epaca_tdnn_soxaug/model/model000000034.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = '/workspace/DATASET/server9_ssd/sdsv21/vox2_trainlist.txt'\n",
    "# train_list = '/workspace/DATASET/server9_ssd/sdsv21/sdsv21_DA_10960.txt'\n",
    "# train_list_FA = '/workspace/DATASET/server9_ssd/sdsv21/sdsv21_normal_FA1417.txt'\n",
    "# train_list_EN = '/workspace/DATASET/server9_ssd/sdsv21/sdsv21_normal_EN9543.txt'\n",
    "train_path = '/workspace/DATASET/server9_ssd/sdsv21'\n",
    "test_list = '/workspace/DATASET/server9_ssd/sdsv21/vox_h_triallist.txt'\n",
    "enroll_list = ''\n",
    "test_path = '/workspace/DATASET/server9_ssd/sdsv21'\n",
    "score_file = '/workspace/LOGS_OUTPUT/server9_nvme1/ASV_LOGS_202102/train_logs_201120/041901/result/eval_scores.txt'\n",
    "result_path = '/workspace/LOGS_OUTPUT/server9_nvme1/ASV_LOGS_202102/train_logs_201120/041901/result'\n",
    "out_path_1 = '/workspace/LOGS_OUTPUT/server9_nvme1/ASV_LOGS_202102/train_logs_201120/041901/result/eval_scores_as1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state = torch.load(model_path, map_location=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_state = self.__model__.module.state_dict()\n",
    "# loaded_state = torch.load(path, map_location=\"cuda:%d\"%self.gpu)\n",
    "# loaded_state = torch.load(path, map_location=\"cpu\")\n",
    "self_state = S.state_dict()\n",
    "\n",
    "for name, param in loaded_state['model'].items():\n",
    "    origname = name\n",
    "\n",
    "    ## pass spk clf weight\n",
    "    if '__L__' in name:\n",
    "        print('pass __L__ classerfier W')\n",
    "        continue\n",
    "\n",
    "    ## pass DA weight\n",
    "    if 'DA_module' in name:\n",
    "        print('pass DA_module params:'+name)\n",
    "        continue\n",
    "\n",
    "    if name not in self_state:\n",
    "        name = name.replace(\"__S__.\", \"\")\n",
    "\n",
    "        if name not in self_state:\n",
    "            print(\"#%s is not in the model.\"%origname)\n",
    "            continue\n",
    "\n",
    "    if self_state[name].size() != loaded_state['model'][origname].size():\n",
    "        print(\"#Wrong parameter length: %s, model: %s, loaded: %s\"%(origname, self_state[name].size(), loaded_state['model'][origname].size()))\n",
    "        continue\n",
    "\n",
    "    self_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWAV(filename, max_frames):\n",
    "\n",
    "    # Maximum audio length\n",
    "    max_audio = max_frames * 160 + 240\n",
    "\n",
    "    # Read wav file and convert to torch tensor\n",
    "    sample_rate, audio  = wavfile.read(filename)\n",
    "\n",
    "    audiosize = audio.shape[0]\n",
    "\n",
    "    if audiosize <= max_audio:\n",
    "        shortage    = max_audio - audiosize + 1 \n",
    "        audio       = numpy.pad(audio, (0, shortage), 'wrap')\n",
    "        audiosize   = audio.shape[0]\n",
    "\n",
    "    \n",
    "    feats = []\n",
    "\n",
    "    feats.append(audio)\n",
    "\n",
    "    feat = numpy.stack(feats,axis=0).astype(numpy.float)\n",
    "\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state['model']['__L__.W'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_matrix = loaded_state['model']['__L__.W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_matrix = W_matrix.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_matrix.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(train_path, train_list)) as dataset_file:\n",
    "    lines = dataset_file.readlines()\n",
    "\n",
    "dictkeys = list(set([x.split()[0] for x in lines]))\n",
    "dictkeys.sort()\n",
    "dictkeys = { key : ii for ii, key in enumerate(dictkeys) }\n",
    "len_dictkeys_ori = len(dictkeys)\n",
    "if True:\n",
    "    len_dictkeys = len(dictkeys)\n",
    "    for ii, key in enumerate(list(dictkeys.keys())):\n",
    "        dictkeys[key+'_slow'] = ii + len_dictkeys\n",
    "        dictkeys[key+'_fast'] = ii + len_dictkeys*2\n",
    "\n",
    "    assert len(dictkeys) == 3*len_dictkeys_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dictkeys.keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dictkeys.keys())[5994-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(train_path, train_list)) as dataset_file:\n",
    "    FA_lines = dataset_file.readlines()\n",
    "FA_dictkeys = list(set([x.split()[0] for x in FA_lines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(FA_dictkeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_list = []\n",
    "for i in FA_dictkeys:\n",
    "    print(dictkeys[i])\n",
    "    FA_list.append(dictkeys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_FA_spk_dict_mean = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, label_num in enumerate(FA_list):\n",
    "    cohort_FA_spk_dict_mean[label_num] = F.normalize(W_matrix[label_num], p=2, dim=0).detach().cpu()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_FA_spk_dict_mean_nm = cohort_FA_spk_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sdsv21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listfilename = test_list\n",
    "files = []\n",
    "with open(listfilename) as listfile:\n",
    "    while True:\n",
    "        line = listfile.readline()\n",
    "        if (not line):\n",
    "            break\n",
    "\n",
    "        data = line.split()\n",
    "\n",
    "        if len(data) == 2: data = [random.randint(0,1)] + data\n",
    "        \n",
    "        files.append(data[1])\n",
    "        files.append(data[2])\n",
    "\n",
    "setfiles = list(set(files))\n",
    "setfiles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(setfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, line in enumerate(setfiles):\n",
    "    wavline = line\n",
    "    wavline = os.path.join(test_path, wavline)\n",
    "    raw_inp = loadWAV(wavline, max_frames=0)\n",
    "    raw_inp = torch.FloatTensor(raw_inp).cuda()\n",
    "    \n",
    "    ref_feat = S.forward(raw_inp).detach().cpu()\n",
    "\n",
    "    test_dict[line] = ref_feat\n",
    "    \n",
    "    if ((count+1) % 1000) == 0:\n",
    "        print((count+1)//1000, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_nm = {}\n",
    "for i in test_dict:\n",
    "    test_dict_nm[i] = F.normalize(test_dict[i].squeeze(0), p=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "# import pickle\n",
    "# with open('tmp_chort_dict', 'wb') as f:\n",
    "#     pickle.dump(cohort_spk_dict, f)\n",
    "# import pickle\n",
    "# with open('tmp_test_dict', 'wb') as f:\n",
    "#     pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4708 * 5994 // 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "cohort2test = {}\n",
    "for i in cohort_FA_spk_dict_mean_nm:\n",
    "    cohort_emb = cohort_FA_spk_dict_mean_nm[i]\n",
    "    for j in test_dict_nm:\n",
    "        test_emb = test_dict_nm[j]\n",
    "        score = F.cosine_similarity(cohort_emb, test_emb, dim=0).numpy().astype(numpy.float16)\n",
    "        cohort2test[str(i)+\" \"+j] = score\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100000) == 0:\n",
    "            print(count // 100000, end='\\r')\n",
    "                \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cohort2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort2test_save_path = os.path.join(result_path, 'tmp_cohort2test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use voxe cohort\n",
    "result_path = '/workspace/LOGS_OUTPUT/server9_nvme1/ASV_LOGS_202102/train_logs_201120/041900/result'\n",
    "cohort2test_save_path = os.path.join(result_path, 'tmp_cohort2test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cohort2test_save_path, 'w') as f:\n",
    "    for count, i in enumerate(cohort2test):\n",
    "        line = '%s %.4f\\n'%(i, cohort2test[i])\n",
    "        f.write(line)\n",
    "        if (count % 100000) == 0:\n",
    "            print(count // 100000, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort2test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional\n",
    "# import pickle\n",
    "# with open('tmp_chort_dict', 'wb') as f:\n",
    "#     pickle.dump(cohort2test, f)\n",
    "# import pickle\n",
    "# with open('tmp_test_dict', 'wb') as f:\n",
    "#     pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "from scipy.io import wavfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def as_norm_1(file_score_path, out_path, cohort2enroll_path, cohort2test_path, top_num=1000, hold_name=True, dict_sep=' '):\n",
    "    # operation: norm_score = 0.5* ((score-mean_e_e)/str_e_e+(score-mean_t_t)/str_t_t)\n",
    "    print('Score norm operation start, method: Adaptive_score_norm_type_1')\n",
    "    print('')\n",
    "    \n",
    "    file_scores_as_norm_path = out_path\n",
    "    enroll_dict = defaultdict(list)\n",
    "    test_dict = defaultdict(list)\n",
    "\n",
    "    with open(cohort2enroll_path, 'r') as f:\n",
    "        for count, i in enumerate(f):\n",
    "            cohort_utt, enroll_utt, score = i.strip().split(dict_sep)\n",
    "            score = float(score)\n",
    "            enroll_dict[enroll_utt].append([cohort_utt, score])\n",
    "            if ((count+1) % 100000) == 0:\n",
    "                print('read c2e:', (count+1)//100000, end='\\r')\n",
    "    \n",
    "#     with open(cohort2test_path, 'r') as f:\n",
    "#         for count, i in enumerate(f):\n",
    "#             cohort_utt, test_utt, score = i.strip().split(dict_sep)\n",
    "#             score = float(score)\n",
    "#             test_dict[test_utt].append([cohort_utt, score])\n",
    "#             if ((count+1) % 100000) == 0:\n",
    "#                 print('read c2t:', (count+1)//100000, end='\\r')\n",
    "                \n",
    "        \n",
    "    mean_e_e = {}\n",
    "    str_e_e = {}\n",
    "    mean_t_t = {}\n",
    "    str_t_t = {}\n",
    "    \n",
    "    print('')\n",
    "    print('calculate enroll statistics')\n",
    "    print('')\n",
    "    \n",
    "    for count, key in enumerate(enroll_dict):\n",
    "        enroll_dict[key] = sorted(enroll_dict[key],key = lambda x:x[1], reverse = True)\n",
    "        tmp_score_list = []\n",
    "        for i in range(top_num):\n",
    "            tmp_score_list.append(enroll_dict[key][i][1])\n",
    "        mean_e_e[key] = numpy.mean(tmp_score_list)\n",
    "        str_e_e[key] = numpy.std(tmp_score_list, ddof=1)\n",
    "        if ((count+1) % 1000) == 0:\n",
    "            print('cal e:', (count+1)//1000, end='\\r')\n",
    "            \n",
    "    mean_t_t = mean_e_e\n",
    "    str_t_t = str_e_e\n",
    "\n",
    "#     for count, key in enumerate(test_dict):\n",
    "#         test_dict[key] = sorted(test_dict[key],key = lambda x:x[1], reverse = True)\n",
    "#         tmp_score_list = []\n",
    "#         for i in range(top_num):\n",
    "#             tmp_score_list.append(test_dict[key][i][1])\n",
    "#         mean_t_t[key] = numpy.mean(tmp_score_list)\n",
    "#         str_t_t[key] = numpy.std(tmp_score_list, ddof=1)\n",
    "#         if ((count+1) % 1000) == 0:\n",
    "#             print('cal t:', (count+1)//1000, end='\\r')\n",
    "\n",
    "    print('')\n",
    "    print('Scoring...')\n",
    "    print('')\n",
    "    \n",
    "    file_scores = open(file_score_path)\n",
    "    with open(file_scores_as_norm_path,'w') as f:\n",
    "        for count, line in enumerate(file_scores):\n",
    "            enroll_utt = line.split(' ')[1].strip()\n",
    "            test_utt = line.split(' ')[2].strip()\n",
    "            score = float(line.split(' ')[0].strip())\n",
    "            norm_score = 0.5 * ((score-mean_e_e[enroll_utt])/str_e_e[enroll_utt]+(score-mean_t_t[test_utt])/str_t_t[test_utt])\n",
    "            if hold_name:\n",
    "                f.write('%.4f %s %s\\n'%(norm_score, enroll_utt, test_utt))\n",
    "            else:\n",
    "                f.write('%.4f\\n'%(norm_score))\n",
    "            \n",
    "            if ((count+1) % 10000) == 0:\n",
    "                print((count+1)//10000, end='\\r')\n",
    "                \n",
    "    file_scores.close()\n",
    "    print('')\n",
    "    print('Adaptive_score_norm_type_1 is finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score norm operation start, method: Adaptive_score_norm_type_1\n",
      "\n",
      "read c2e: 8700\n",
      "calculate enroll statistics\n",
      "\n",
      "cal e: 145\n",
      "Scoring...\n",
      "\n",
      "55\n",
      "Adaptive_score_norm_type_1 is finished\n"
     ]
    }
   ],
   "source": [
    "as_norm_1(score_file, out_path_1, cohort2test_save_path, cohort2test_save_path, top_num=1500, hold_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# as_norm_2(score_file, out_path_2, cohort2test, cohort2test, top_num=1000, hold_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get asnormed score_file & scoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
