{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import torch\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "from scipy.io import wavfile\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from asnorm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cohort set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/workspace/GREAT_ASV_system/train_dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from DatasetLoader import loadWAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpeakerNetModel = importlib.import_module('models.'+'EPACA-TDNN').__getattribute__('MainModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPACA-TDNN.py, Embedding size is 192, Channels 1024, Spec_aug False.\n"
     ]
    }
   ],
   "source": [
    "# EPACA-TDNN\n",
    "S = SpeakerNetModel(n_mels=40, nOut=192, spec_aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/workspace/LOGS_OUTPUT/tmp_logs/train_logs_201120/multi_gpu_epaca_tdnn_soxaug/model/model000000034.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = '/workspace/DATASET/server9_ssd/voxceleb/vox2_trainlist.txt'\n",
    "train_path = '/workspace/DATASET/server9_ssd/voxceleb'\n",
    "test_list = '/workspace/DATASET/server9_ssd/voxceleb/voxsrc2020_final_pairs.txt'\n",
    "test_path = '/workspace/DATASET/server9_ssd/voxceleb/voxsrc2020'\n",
    "score_file = '/workspace/LOGS_OUTPUT/tmp_logs/train_logs_201120/multi_gpu_epaca_tdnn_soxaug_voxsrc20/result/eval_scores.txt'\n",
    "out_path_1 = 'as1.txt'\n",
    "# out_path_2 = 'as2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ECAPA_TDNN(\n",
       "  (instancenorm): InstanceNorm1d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (torchfb): Sequential(\n",
       "    (0): PreEmphasis()\n",
       "    (1): MelSpectrogram(\n",
       "      (spectrogram): Spectrogram()\n",
       "      (mel_scale): MelScale()\n",
       "    )\n",
       "  )\n",
       "  (layer1): Conv1dReluBn(\n",
       "    (conv): Conv1d(40, 1024, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Res2Conv1dReluBn(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "      )\n",
       "      (bns): ModuleList(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): SE_Connect(\n",
       "      (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Res2Conv1dReluBn(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "      )\n",
       "      (bns): ModuleList(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): SE_Connect(\n",
       "      (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Res2Conv1dReluBn(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "      )\n",
       "      (bns): ModuleList(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): SE_Connect(\n",
       "      (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (conv): Conv1d(3072, 3072, kernel_size=(1,), stride=(1,))\n",
       "  (pooling): AttentiveStatsPool(\n",
       "    (linear1): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
       "    (linear2): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (bn1): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=6144, out_features=192, bias=True)\n",
       "  (bn2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state = torch.load(model_path, map_location=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#__L__.W is not in the model.\n"
     ]
    }
   ],
   "source": [
    "self_state = S.state_dict()\n",
    "\n",
    "for name, param in loaded_state['model'].items():\n",
    "    origname = name\n",
    "    if name not in self_state:\n",
    "        name = name.replace(\"__S__.\", \"\")\n",
    "\n",
    "        if name not in self_state:\n",
    "            print(\"#%s is not in the model.\"%origname)\n",
    "            continue\n",
    "\n",
    "    if self_state[name].size() != loaded_state['model'][origname].size():\n",
    "        print(\"#Wrong parameter length: %s, model: %s, loaded: %s\"%(origname, self_state[name].size(), loaded_state[origname].size()))\n",
    "        continue\n",
    "\n",
    "    self_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ECAPA_TDNN(\n",
       "  (instancenorm): InstanceNorm1d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (torchfb): Sequential(\n",
       "    (0): PreEmphasis()\n",
       "    (1): MelSpectrogram(\n",
       "      (spectrogram): Spectrogram()\n",
       "      (mel_scale): MelScale()\n",
       "    )\n",
       "  )\n",
       "  (layer1): Conv1dReluBn(\n",
       "    (conv): Conv1d(40, 1024, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Res2Conv1dReluBn(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "        (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
       "      )\n",
       "      (bns): ModuleList(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): SE_Connect(\n",
       "      (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Res2Conv1dReluBn(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "        (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
       "      )\n",
       "      (bns): ModuleList(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): SE_Connect(\n",
       "      (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Res2Conv1dReluBn(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "        (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
       "      )\n",
       "      (bns): ModuleList(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Conv1dReluBn(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): SE_Connect(\n",
       "      (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (conv): Conv1d(3072, 3072, kernel_size=(1,), stride=(1,))\n",
       "  (pooling): AttentiveStatsPool(\n",
       "    (linear1): Conv1d(3072, 128, kernel_size=(1,), stride=(1,))\n",
       "    (linear2): Conv1d(128, 3072, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (bn1): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=6144, out_features=192, bias=True)\n",
       "  (bn2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWAV(filename, max_frames):\n",
    "\n",
    "    # Maximum audio length\n",
    "    max_audio = max_frames * 160 + 240\n",
    "\n",
    "    # Read wav file and convert to torch tensor\n",
    "    sample_rate, audio  = wavfile.read(filename)\n",
    "\n",
    "    audiosize = audio.shape[0]\n",
    "\n",
    "    if audiosize <= max_audio:\n",
    "        shortage    = max_audio - audiosize + 1 \n",
    "        audio       = numpy.pad(audio, (0, shortage), 'wrap')\n",
    "        audiosize   = audio.shape[0]\n",
    "\n",
    "    \n",
    "    feats = []\n",
    "\n",
    "    feats.append(audio)\n",
    "\n",
    "    feat = numpy.stack(feats,axis=0).astype(numpy.float)\n",
    "\n",
    "    return feat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_list, 'r') as f:\n",
    "    train_list_list = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1092009"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092\r"
     ]
    }
   ],
   "source": [
    "cohort_spk_dict = {}\n",
    "for count, line in enumerate(train_list_list):\n",
    "    label, wavline = line[:-1].split(' ')\n",
    "    wavline = os.path.join(train_path, wavline)\n",
    "    raw_inp = loadWAV(wavline, max_frames=0)\n",
    "    raw_inp = torch.FloatTensor(raw_inp).cuda()\n",
    "    \n",
    "    ref_feat = S.forward(raw_inp).detach().cpu()\n",
    "    if label not in cohort_spk_dict.keys():\n",
    "        cohort_spk_dict[label] = ref_feat\n",
    "    else:\n",
    "        cohort_spk_dict[label] = torch.cat([cohort_spk_dict[label], ref_feat], dim=0)\n",
    "    if ((count+1) % 1000) == 0:\n",
    "        print((count+1)//1000, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_spk_dict_mean = {}\n",
    "for i in cohort_spk_dict:\n",
    "    cohort_spk_dict_mean[i] = torch.mean(cohort_spk_dict[i], dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_spk_dict_mean_nm = {}\n",
    "for i in cohort_spk_dict_mean:\n",
    "    cohort_spk_dict_mean_nm[i] = F.normalize(cohort_spk_dict_mean[i], p=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5994"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cohort_spk_dict_mean_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract vox-src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "listfilename = test_list\n",
    "files = []\n",
    "with open(listfilename) as listfile:\n",
    "    while True:\n",
    "        line = listfile.readline()\n",
    "        if (not line):\n",
    "            break\n",
    "\n",
    "        data = line.split()\n",
    "\n",
    "        files.append(data[0])\n",
    "        files.append(data[1])\n",
    "\n",
    "setfiles = list(set(files))\n",
    "setfiles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118439"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(setfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\r"
     ]
    }
   ],
   "source": [
    "for count, line in enumerate(setfiles):\n",
    "    wavline = line\n",
    "    wavline = os.path.join(test_path, wavline)\n",
    "    raw_inp = loadWAV(wavline, max_frames=0)\n",
    "    raw_inp = torch.FloatTensor(raw_inp).cuda()\n",
    "    \n",
    "    ref_feat = S.forward(raw_inp).detach().cpu()\n",
    "\n",
    "    test_dict[line] = ref_feat\n",
    "    \n",
    "    if ((count+1) % 1000) == 0:\n",
    "        print((count+1)//1000, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118439"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_nm = {}\n",
    "for i in test_dict:\n",
    "    test_dict_nm[i] = F.normalize(test_dict[i].squeeze(0), p=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "# import pickle\n",
    "# with open('tmp_chort_dict', 'wb') as f:\n",
    "#     pickle.dump(cohort_spk_dict, f)\n",
    "# import pickle\n",
    "# with open('tmp_test_dict', 'wb') as f:\n",
    "#     pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7099"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "118439 * 5994 // 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709923366\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "cohort2test = {}\n",
    "for i in cohort_spk_dict_mean_nm:\n",
    "    cohort_emb = cohort_spk_dict_mean_nm[i]\n",
    "    for j in test_dict_nm:\n",
    "        test_emb = test_dict_nm[j]\n",
    "        score = F.cosine_similarity(cohort_emb, test_emb, dim=0).cpu().numpy().astype(numpy.float16)\n",
    "        cohort2test[i+\" \"+j] = score\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100000) == 0:\n",
    "            print(count // 100000, end='\\r')\n",
    "                \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709923366"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cohort2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort2test_save_path = 'tmp_cohort2test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7099\r"
     ]
    }
   ],
   "source": [
    "with open(cohort2test_save_path, 'w') as f:\n",
    "    for count, i in enumerate(cohort2test):\n",
    "        line = '%s %.4f\\n'%(i, cohort2test[i])\n",
    "        f.write(line)\n",
    "        if (count % 100000) == 0:\n",
    "            print(count // 100000, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional\n",
    "# import pickle\n",
    "# with open('tmp_chort_dict', 'wb') as f:\n",
    "#     pickle.dump(cohort2test, f)\n",
    "# import pickle\n",
    "# with open('tmp_test_dict', 'wb') as f:\n",
    "#     pickle.dump(test_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "from scipy.io import wavfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def as_norm_1(file_score_path, out_path, cohort2enroll_path, cohort2test_path, top_num=200, hold_name=True, dict_sep=' '):\n",
    "    # operation: norm_score = 0.5* ((score-mean_e_e)/str_e_e+(score-mean_t_t)/str_t_t)\n",
    "    print('Score norm operation start, method: Adaptive_score_norm_type_1')\n",
    "    print('')\n",
    "    \n",
    "    file_scores_as_norm_path = out_path\n",
    "    enroll_dict = defaultdict(list)\n",
    "    test_dict = defaultdict(list)\n",
    "    \n",
    "    with open(cohort2test_path, 'r') as f:\n",
    "        for count, i in enumerate(f):\n",
    "            cohort_utt, enroll_utt, score = i.strip().split(dict_sep)\n",
    "            score = float(score)\n",
    "            enroll_dict[enroll_utt].append([cohort_utt, score])\n",
    "            if ((count+1) % 100000) == 0:\n",
    "                print('read c2e:', (count+1)//100000, end='\\r')\n",
    "#     for count, i in enumerate(cohort2enroll):\n",
    "#         cohort_utt = i.split(dict_sep)[0]\n",
    "#         enroll_utt = i.split(dict_sep)[1]\n",
    "#         score = float(cohort2enroll[i])\n",
    "#         enroll_dict[enroll_utt].append([cohort_utt, score])\n",
    "\n",
    "#         if ((count+1) % 100000) == 0:\n",
    "#             print('read c2e:', (count+1)//100000, end='\\r')\n",
    "            \n",
    "    test_dict = enroll_dict\n",
    "        \n",
    "    mean_e_e = {}\n",
    "    str_e_e = {}\n",
    "    mean_t_t = {}\n",
    "    str_t_t = {}\n",
    "    \n",
    "    print('calculate all statistics')\n",
    "    print('')\n",
    "    \n",
    "    for count, key in enumerate(enroll_dict):\n",
    "        enroll_dict[key] = sorted(enroll_dict[key],key = lambda x:x[1], reverse = True)\n",
    "        tmp_score_list = []\n",
    "        for i in range(top_num):\n",
    "            tmp_score_list.append(enroll_dict[key][i][1])\n",
    "        mean_e_e[key] = numpy.mean(tmp_score_list)\n",
    "        str_e_e[key] = numpy.std(tmp_score_list, ddof=1)\n",
    "        if ((count+1) % 1000) == 0:\n",
    "            print('cal e:', (count+1)//1000, end='\\r')\n",
    "        \n",
    "    mean_t_t = mean_e_e\n",
    "    str_t_t = str_e_e\n",
    "\n",
    "    print('Scoring...')\n",
    "    print('')\n",
    "    \n",
    "    file_scores = open(file_score_path)\n",
    "    with open(file_scores_as_norm_path,'w') as f:\n",
    "        for count, line in enumerate(file_scores):\n",
    "            enroll_utt = line.split(' ')[1].strip()\n",
    "            test_utt = line.split(' ')[2].strip()\n",
    "            score = float(line.split(' ')[0].strip())\n",
    "            norm_score = 0.5 * ((score-mean_e_e[enroll_utt])/str_e_e[enroll_utt]+(score-mean_t_t[test_utt])/str_t_t[test_utt])\n",
    "            if hold_name:\n",
    "                f.write('%.4f %s %s\\n'%(norm_score, enroll_utt, test_utt))\n",
    "            else:\n",
    "                f.write('%.4f\\n'%(norm_score))\n",
    "            \n",
    "            if ((count+1) % 10000) == 0:\n",
    "                print((count+1)//10000, end='\\r')\n",
    "                \n",
    "    file_scores.close()\n",
    "    \n",
    "    print('Adaptive_score_norm_type_1 is finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score norm operation start, method: Adaptive_score_norm_type_1\n",
      "calculate all statistics2838 2841 2844 2856 2864 2870 2900 2921 2923 2928 3002 3012 3018 3031 3053 3078 3824 3837 4240 4260 4565 4608 4647 4657 4870 4902 4948 5030 5066 5147 5153 5211 5226 5233 5248 5289 5296 5299 5317 5377 5403 5409 5419 5432 5442 5449 5465 5472 5585 5590 5594 5617 5621 5625 5639 5656 5679 5703 5718 5731 5744 5765 5769 5784 5795 5798 5849 5859 5863 5886 5903 5942 5987 5990 5994 5996 6010 6013 6037 6054 6098 6105 6156 6207 6220 6224 6228 6233 6258 6271 6393 6397 6445 6466 6552 6555 6575 6580 6586 6600 6640 6670 6693\n",
      "Scoring...\n",
      "Adaptive_score_norm_type_1 is finished\n"
     ]
    }
   ],
   "source": [
    "as_norm_1(score_file, out_path_1, cohort2test_save_path, cohort2test_save_path, top_num=200, hold_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# as_norm_2(score_file, out_path_2, cohort2test, cohort2test, top_num=1000, hold_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get asnormed score_file & scoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
